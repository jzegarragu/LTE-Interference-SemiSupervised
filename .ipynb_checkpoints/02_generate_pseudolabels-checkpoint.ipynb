{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELDA 1: Markdown de Introducción\n",
    "# ==============================================================================\n",
    "# ## ETAPA 2: Generación de Pseudo-Etiquetas\n",
    "#\n",
    "# Este notebook carga los modelos \"Teacher\" entrenados en la etapa anterior y los \n",
    "# utiliza para realizar inferencia sobre el conjunto de datos no etiquetado (`SIN_CLASIFICAR`).\n",
    "#\n",
    "# Las predicciones que superen un umbral de confianza (`THRESHOLD`) se guardarán \n",
    "# como \"pseudo-etiquetas\" en una nueva estructura de carpetas (`PSEUDO`), \n",
    "# que servirá para aumentar el dataset de entrenamiento del \"Student\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELDA 2: Importaciones\n",
    "# ==============================================================================\n",
    "# --- Importaciones ---\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# --- Importar desde nuestros módulos locales ---\n",
    "import config\n",
    "import models\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELDA 3: Clases de Dataset y Lógica de Inferencia\n",
    "# ==============================================================================\n",
    "# ### Clases de Dataset y Lógica de Inferencia\n",
    "\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, root: Path, transform=None):\n",
    "        self.paths = sorted(list(root.glob(\"**/*.png\")))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img = Image.open(self.paths[i]).convert(\"L\")\n",
    "        if self.transform:\n",
    "            x = self.transform(img)\n",
    "        else:\n",
    "            x = transforms.ToTensor()(img)\n",
    "        return x, str(self.paths[i])\n",
    "\n",
    "def predict_and_save(model_name, params, ckpt_path, unlabeled_loader, class_names, threshold=0.8):\n",
    "    device = torch.device(config.DEVICE)\n",
    "    num_classes = len(class_names)\n",
    "    idx2cls = {i: name for i, name in enumerate(class_names)}\n",
    "\n",
    "    # Construir el modelo y cargar los pesos del checkpoint\n",
    "    model = models.make_model(model_name, num_classes, params.get('RESNET_USE_PRETRAIN', True)).to(device)\n",
    "    model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    all_rows = []\n",
    "    with torch.no_grad():\n",
    "        for xb, paths in unlabeled_loader:\n",
    "            # Reutilizar la función de utils para el redimensionamiento\n",
    "            # Asumiendo que utils.py tiene la función maybe_resize_for_resnet\n",
    "            # Si no, puedes definirla aquí también.\n",
    "            should_resize = params.get('RESNET_RESIZE_TO_224', False)\n",
    "            if 'maybe_resize_for_resnet' in dir(utils):\n",
    "                 xb = utils.maybe_resize_for_resnet(xb, should_resize)\n",
    "            elif should_resize:\n",
    "                 xb = torch.nn.functional.interpolate(xb, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            conf, pred = probs.max(dim=1)\n",
    "\n",
    "            for pth, yhat, c in zip(paths, pred.cpu().numpy(), conf.cpu().numpy()):\n",
    "                cls_name = idx2cls[int(yhat)]\n",
    "                passed = float(c) >= threshold\n",
    "                all_rows.append({\n",
    "                    \"img_path\": pth,\n",
    "                    \"pred_class\": cls_name,\n",
    "                    \"confidence\": float(c),\n",
    "                    \"pass_threshold\": bool(passed)\n",
    "                })\n",
    "                \n",
    "                if passed:\n",
    "                    dest_dir = config.PSEUDO_DIR / model_name / cls_name\n",
    "                    dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "                    shutil.copy(pth, dest_dir / Path(pth).name)\n",
    "    \n",
    "    df = pd.DataFrame(all_rows)\n",
    "    csv_path = config.RESULTS_DIR / f\"pseudolabels_{model_name}.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Resultados de pseudo-etiquetado guardados en {csv_path}\")\n",
    "    \n",
    "    # Imprimir resumen\n",
    "    accepted_count = df['pass_threshold'].sum()\n",
    "    print(f\"Total de pseudo-etiquetas generadas por '{model_name}': {accepted_count}\")\n",
    "    if accepted_count > 0:\n",
    "        print(\"Distribución por clase:\")\n",
    "        print(df[df['pass_threshold']]['pred_class'].value_counts())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontradas 643 imágenes en 'SIN_CLASIFICAR'\n",
      "\n",
      "==================== GENERANDO PSEUDO-ETIQUETAS CON: cnn_paper ====================\n",
      "[ERROR] No se encontró el checkpoint: D:\\PYTHON\\30_CLASIFICADOR_DE_INTERFERENCIAS\\CHECKPOINTS\\cnn_paper_best_Carrier_C3_2975.pth. Saltando este modelo.\n",
      "\n",
      "==================== GENERANDO PSEUDO-ETIQUETAS CON: cnn_paper_L2 ====================\n",
      "[ERROR] No se encontró el checkpoint: D:\\PYTHON\\30_CLASIFICADOR_DE_INTERFERENCIAS\\CHECKPOINTS\\cnn_paper_L2_best_Carrier_C3_2975.pth. Saltando este modelo.\n",
      "\n",
      "==================== GENERANDO PSEUDO-ETIQUETAS CON: resnet50 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jorge Zegarra\\AppData\\Local\\Temp\\ipykernel_2464\\2943864742.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de pseudo-etiquetado guardados en D:\\PYTHON\\30_CLASIFICADOR_DE_INTERFERENCIAS\\RESULTADOS\\Carrier_C3_2975\\pseudolabels_resnet50.csv\n",
      "Total de pseudo-etiquetas generadas por 'resnet50': 619\n",
      "Distribución por clase:\n",
      "pred_class\n",
      "PIM_OTRO       303\n",
      "TINA           184\n",
      "ARM_DELGADO     81\n",
      "WIFI            40\n",
      "MW              11\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELDA 4: Bucle Principal de Inferencia\n",
    "# ==============================================================================\n",
    "# ### Bucle Principal de Inferencia\n",
    "\n",
    "# --- Preparar Dataloader para datos no etiquetados ---\n",
    "pred_tfms = transforms.Compose([\n",
    "    transforms.Resize((config.IMG_H, config.IMG_W)),\n",
    "    transforms.ToTensor(),\n",
    "    # La normalización debe ser consistente con el entrenamiento si se usó\n",
    "    # Si no, a veces es mejor no normalizar para la inferencia simple.\n",
    "    # transforms.Normalize(mean=[0.5], std=[0.5]), \n",
    "])\n",
    "\n",
    "unlabeled_ds = UnlabeledDataset(config.UNLABELED_DIR, transform=pred_tfms)\n",
    "unlabeled_loader = DataLoader(unlabeled_ds, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Encontradas {len(unlabeled_ds)} imágenes en '{config.UNLABELED_DIR.name}'\")\n",
    "\n",
    "class_names = sorted([p.name for p in config.TRAIN_VAL_DIR.iterdir() if p.is_dir()])\n",
    "\n",
    "# --- Bucle de inferencia para cada Teacher ---\n",
    "for model_key, params in config.TRAIN_PARAMS.items():\n",
    "    print(f\"\\n{'='*20} GENERANDO PSEUDO-ETIQUETAS CON: {model_key} {'='*20}\")\n",
    "    \n",
    "    ckpt_path = config.CHECKPOINTS_DIR / f\"{params['MODEL_NAME']}_best_{config.CARRIER}.pth\"\n",
    "    if not ckpt_path.exists():\n",
    "        print(f\"[ERROR] No se encontró el checkpoint: {ckpt_path}. Saltando este modelo.\")\n",
    "        continue\n",
    "        \n",
    "    # Limpiar directorio de pseudo-etiquetas anterior para este modelo\n",
    "    pseudo_model_dir = config.PSEUDO_DIR / model_key\n",
    "    if pseudo_model_dir.exists():\n",
    "        print(f\"Limpiando directorio anterior: {pseudo_model_dir}\")\n",
    "        shutil.rmtree(pseudo_model_dir)\n",
    "    \n",
    "    predict_and_save(\n",
    "        model_name=params['MODEL_NAME'],\n",
    "        params=params,\n",
    "        ckpt_path=ckpt_path,\n",
    "        unlabeled_loader=unlabeled_loader,\n",
    "        class_names=class_names,\n",
    "        threshold=0.50 # Puedes mover esto a config.py si quieres\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "from_bard": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
