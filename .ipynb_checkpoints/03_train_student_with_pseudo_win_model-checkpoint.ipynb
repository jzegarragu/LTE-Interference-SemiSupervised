{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26ee4ccb-1e88-4dbd-8367-58ca0943191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELDA 1: Markdown de IntroducciÃ³n\n",
    "# ==============================================================================\n",
    "# ## ETAPA 3: Entrenamiento del \"Student\"\n",
    "#\n",
    "# Este notebook entrena el modelo final, o \"Student\". Su principal caracterÃ­stica \n",
    "# es que utiliza un **conjunto de datos de entrenamiento aumentado**.\n",
    "#\n",
    "# Este conjunto de datos se compone de:\n",
    "# 1. Los datos de entrenamiento originales (del `train_val` split).\n",
    "# 2. Las nuevas pseudo-etiquetas de alta calidad generadas por los \"Teachers\" y **verificadas por un experto**.\n",
    "#\n",
    "# El objetivo es que el \"Student\", al aprender de mÃ¡s datos, logre un mejor \n",
    "# rendimiento y capacidad de generalizaciÃ³n que los \"Teachers\" originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1d37a55-8dbf-4ebf-bdf0-dc0527ac7123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELDA 2: Importaciones\n",
    "# ==============================================================================\n",
    "# --- Importaciones EstÃ¡ndar ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix\n",
    "import time\n",
    "from pathlib import Path\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# --- Importar desde nuestros mÃ³dulos locales ---\n",
    "import config\n",
    "import models\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c7a20ec-0b4a-4731-ac9e-03cc47d1748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELDA 2.5: Clase Asistente para Tesis (ThesisHelper)\n",
    "# ==============================================================================\n",
    "class ThesisHelper:\n",
    "    \"\"\"\n",
    "    Una clase para gestionar el logging, guardado de checkpoints y generaciÃ³n\n",
    "    de artefactos para una tesis durante el entrenamiento de un modelo.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, class_names, base_dir, run_type='teacher'):\n",
    "        self.params = params\n",
    "        self.class_names = class_names\n",
    "        # El nombre ahora incluye el tipo de ejecuciÃ³n (teacher o student)\n",
    "        if run_type == 'student':\n",
    "            self.run_name = f\"student_trained_with_win_teachers{params['MODEL_NAME']}\"\n",
    "        else: # Para los 'teacher'\n",
    "            self.run_name = f\"{run_type}_{params['MODEL_NAME']}\"\n",
    "        self.output_dir = Path(base_dir) / self.run_name\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.run_type = run_type # Guardamos el tipo\n",
    "        \n",
    "        self.history = []\n",
    "        self.best_f1_macro = -1.0\n",
    "        self.best_epoch_metrics = None\n",
    "        \n",
    "        print(f\"ThesisHelper inicializado para '{self.run_name}'. Artefactos se guardarÃ¡n en: {self.output_dir}\")\n",
    "\n",
    "\n",
    "    def log_epoch(self, model, metrics):\n",
    "        \"\"\"Registra las mÃ©tricas de una Ã©poca y guarda el mejor checkpoint.\"\"\"\n",
    "        self.history.append(metrics)\n",
    "        current_f1_macro = metrics['f1m']\n",
    "        \n",
    "        if current_f1_macro > self.best_f1_macro:\n",
    "            self.best_f1_macro = current_f1_macro\n",
    "            self.best_epoch_metrics = metrics\n",
    "            print(f\"ðŸš€ Nuevo mejor F1-Macro: {self.best_f1_macro:.4f} en la Ã©poca {metrics['epoch']}. Guardando checkpoint...\")\n",
    "            self._save_checkpoint(model)\n",
    "\n",
    "    def _save_checkpoint(self, model):\n",
    "        \"\"\"Guarda el state_dict del modelo.\"\"\"\n",
    "        torch.save(model.state_dict(), self.output_dir / 'best_model.pth')\n",
    "\n",
    "    def finalize(self, total_duration_seconds):\n",
    "        \"\"\"Genera todos los artefactos finales despuÃ©s del entrenamiento.\"\"\"\n",
    "        if not self.history:\n",
    "            print(\"No hay historial para finalizar. Saltando la generaciÃ³n de artefactos.\")\n",
    "            return\n",
    "\n",
    "        # 1. Guardar historial y resumen\n",
    "        history_df = pd.DataFrame(self.history)\n",
    "        history_df.to_csv(self.output_dir / 'training_history.csv', index=False)\n",
    "        \n",
    "        summary = self.best_epoch_metrics.copy()\n",
    "        summary['total_duration_min'] = total_duration_seconds / 60\n",
    "        # La matriz de confusiÃ³n puede ser grande, la guardamos por separado\n",
    "        cm = summary.pop('cm', None) \n",
    "        \n",
    "        with open(self.output_dir / 'summary.json', 'w') as f:\n",
    "            json.dump(summary, f, indent=4)\n",
    "            \n",
    "        print(f\"ðŸ“„ Historial y resumen guardados.\")\n",
    "\n",
    "        # 2. Generar y guardar grÃ¡ficas\n",
    "        self._plot_curves(history_df)\n",
    "        print(f\"ðŸ“Š GrÃ¡ficas de entrenamiento guardadas.\")\n",
    "        \n",
    "        # 3. Generar y guardar tabla LaTeX\n",
    "        self._generate_latex_table(summary, cm)\n",
    "        print(f\"ðŸ“‹ Tabla LaTeX generada.\")\n",
    "        \n",
    "        # 4. Registrar en el Excel principal\n",
    "        self._log_to_excel(summary, cm)\n",
    "        print(f\"âœ… MÃ©tricas finales registradas en Excel.\")\n",
    "\n",
    "    def _plot_curves(self, df):\n",
    "        \"\"\"Genera y guarda las curvas de entrenamiento y validaciÃ³n.\"\"\"\n",
    "        best_epoch = self.best_epoch_metrics['epoch']\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "        fig.suptitle(f'Curvas de Entrenamiento para {self.run_name}', fontsize=16)\n",
    "\n",
    "        # Subplot 1: PÃ©rdidas (Loss)\n",
    "        ax1.plot(df['epoch'], df['tr_loss'], 'o-', label='Training Loss')\n",
    "        ax1.plot(df['epoch'], df['loss'], 'o-', label='Validation Loss')\n",
    "        ax1.axvline(x=best_epoch, color='r', linestyle='--', label=f'Mejor Ã‰poca ({best_epoch})')\n",
    "        ax1.set_ylabel('PÃ©rdida (Loss)')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "        \n",
    "        # AnotaciÃ³n para la mejor Ã©poca\n",
    "        best_loss = self.best_epoch_metrics['loss']\n",
    "        ax1.annotate(f'Mejor F1-Macro\\nVal Loss: {best_loss:.4f}',\n",
    "                     xy=(best_epoch, best_loss),\n",
    "                     xytext=(best_epoch + 3, best_loss + 0.1*df['loss'].max()),\n",
    "                     arrowprops=dict(facecolor='black', shrink=0.05, width=1, headwidth=8),\n",
    "                     bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"yellow\", ec=\"black\", lw=1, alpha=0.7))\n",
    "\n",
    "        # Subplot 2: MÃ©tricas de Rendimiento\n",
    "        ax2.plot(df['epoch'], df['tr_acc'], 'o-', label='Training Accuracy')\n",
    "        ax2.plot(df['epoch'], df['acc'], 'o-', label='Validation Accuracy')\n",
    "        ax2.plot(df['epoch'], df['f1m'], 'o-', label='Validation F1-Macro', linewidth=2, markersize=8)\n",
    "        ax2.axvline(x=best_epoch, color='r', linestyle='--')\n",
    "        ax2.set_xlabel('Ã‰poca')\n",
    "        ax2.set_ylabel('MÃ©trica')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, linestyle='--', alpha=0.6)\n",
    "        ax2.yaxis.set_major_formatter(mticker.PercentFormatter(xmax=1.0))\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.savefig(self.output_dir / 'training_curves.png', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def _generate_latex_table(self, summary, cm):\n",
    "        \"\"\"Genera una tabla LaTeX con el resumen del entrenamiento.\"\"\"\n",
    "        latex_str = f\"\"\"\n",
    "\\\\begin{{table}}[h!]\n",
    "\\\\centering\n",
    "\\\\caption{{Resumen del entrenamiento del modelo {self.run_name.replace('_', ' ')} y mÃ©tricas finales en la mejor Ã©poca.}}\n",
    "\\\\label{{tab:training_summary_{self.run_name}}}\n",
    "\\\\begin{{tabular}}{{ll}}\n",
    "\\\\hline\n",
    "\\\\textbf{{ParÃ¡metro}} & \\\\textbf{{Valor}} \\\\\\\\\n",
    "\\\\hline\n",
    "Modelo Base & {self.params['MODEL_NAME']} \\\\\\\\\n",
    "Mejor Ã‰poca & {summary['epoch']} \\\\\\\\\n",
    "DuraciÃ³n Total (min) & {summary['total_duration_min']:.2f} \\\\\\\\\n",
    "\\\\hline\n",
    "\\\\textbf{{MÃ©trica de ValidaciÃ³n}} & \\\\textbf{{Valor}} \\\\\\\\\n",
    "\\\\hline\n",
    "F1-Macro (Mejor) & {summary['f1m']:.4f} \\\\\\\\\n",
    "Exactitud (Accuracy) & {summary['acc']:.4f} \\\\\\\\\n",
    "PÃ©rdida (Loss) & {summary['loss']:.4f} \\\\\\\\\n",
    "Recall (Macro) & {summary['recm']:.4f} \\\\\\\\\n",
    "\\\\hline\n",
    "\\\\end{{tabular}}\n",
    "\\\\end{{table}}\n",
    "        \"\"\"\n",
    "        with open(self.output_dir / 'summary_table.tex', 'w') as f:\n",
    "            f.write(latex_str)\n",
    "\n",
    "    def _log_to_excel(self, summary, cm):\n",
    "        \"\"\"Registra las mÃ©tricas en el archivo Excel principal.\"\"\"\n",
    "        metrics_to_log = {\n",
    "            'carrier': config.CARRIER,\n",
    "            'model_name': self.params['MODEL_NAME'],\n",
    "            'run_tag': self.run_name,\n",
    "            'num_classes': len(self.class_names),\n",
    "            'acc': summary['acc'],\n",
    "            'loss': summary['loss'],\n",
    "            'f1m': summary['f1m'],\n",
    "            'f1w': summary['f1w'],\n",
    "            'recm': summary['recm'],\n",
    "            'cm': cm,\n",
    "            'epochs': self.params['EPOCHS'],\n",
    "            'batch_size': self.params['BATCH_SIZE'],\n",
    "            'lr': self.params['LR'],\n",
    "            'weight_decay': self.params['WEIGHT_DECAY'],\n",
    "            'notes': f\"Student - Mejor checkpoint en la Ã©poca {summary['epoch']}\"\n",
    "        }\n",
    "        utils.log_metrics_excel(config.EXCEL_PATH, config.RESULTS_DIR, self.class_names, metrics_to_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50465324-8b35-4418-a110-8fa750ca4942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELDA 3: ReutilizaciÃ³n de Clases y Funciones\n",
    "# ==============================================================================\n",
    "# Reutilizamos las mismas clases y funciones de entrenamiento del notebook 01\n",
    "# ya que la lÃ³gica de entrenamiento es idÃ©ntica.\n",
    "\n",
    "class RandomTimeShift(torch.nn.Module):\n",
    "    def __init__(self, max_frac=0.1):\n",
    "        super().__init__()\n",
    "        self.max_frac = max_frac\n",
    "    def forward(self, x):\n",
    "        _, H, W = x.shape\n",
    "        s = int(random.uniform(-self.max_frac, self.max_frac) * W)\n",
    "        return torch.roll(x, shifts=s, dims=-1)\n",
    "\n",
    "class RandomGain(torch.nn.Module):\n",
    "    def __init__(self, a=0.95, b=1.05):\n",
    "        super().__init__()\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "    def forward(self, x):\n",
    "        g = random.uniform(self.a, self.b)\n",
    "        return (x * g).clamp(0, 1)\n",
    "\n",
    "weak_aug = T.Compose([\n",
    "    RandomTimeShift(0.08),\n",
    "    RandomGain(0.95, 1.05),\n",
    "])\n",
    "\n",
    "class LabeledSpectro(Dataset):\n",
    "    def __init__(self, files, labels, transform=None):\n",
    "        self.files = files\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    def __getitem__(self, i):\n",
    "        path = self.files[i]\n",
    "        try:\n",
    "            x = utils.load_png_gray(path)\n",
    "            if self.transform:\n",
    "                x = self.transform(x)\n",
    "            y = self.labels[i]\n",
    "            return x, y\n",
    "        except Exception as e:\n",
    "            print(f\"[dataset_warning] Saltando archivo por error: {e}\")\n",
    "            return torch.zeros(1, config.IMG_H, config.IMG_W, dtype=torch.float32), self.labels[i]\n",
    "\n",
    "def maybe_resize_for_resnet(x, should_resize):\n",
    "    if should_resize:\n",
    "        return torch.nn.functional.interpolate(x, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "    return x\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience, min_delta, mode='max', restore_best=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.restore_best = restore_best\n",
    "        self.best = -float('inf') if mode == 'max' else float('inf')\n",
    "        self.wait = 0\n",
    "        self.best_state = None\n",
    "\n",
    "    def step(self, metric, model):\n",
    "        is_better = (metric > self.best + self.min_delta) if self.mode == 'max' else (metric < self.best - self.min_delta)\n",
    "        if is_better:\n",
    "            self.best = metric\n",
    "            self.wait = 0\n",
    "            if self.restore_best:\n",
    "                self.best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            return False\n",
    "        self.wait += 1\n",
    "        return self.wait >= self.patience\n",
    "\n",
    "    def restore(self, model):\n",
    "        if self.restore_best and self.best_state is not None:\n",
    "            model.load_state_dict(self.best_state)\n",
    "\n",
    "def evaluate(model, loader, criterion, params):\n",
    "    device = torch.device(config.DEVICE)\n",
    "    model.eval()\n",
    "    va_loss, preds, gts = 0.0, [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = maybe_resize_for_resnet(xb, params.get('RESNET_RESIZE_TO_224', False))\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            va_loss += loss.item() * xb.size(0)\n",
    "            preds.append(logits.softmax(1).argmax(1).cpu())\n",
    "            gts.append(yb.cpu())\n",
    "    va_loss /= len(loader.dataset)\n",
    "    y_pred = torch.cat(preds).numpy()\n",
    "    y_true = torch.cat(gts).numpy()\n",
    "    \n",
    "    metrics = {\n",
    "        'loss': va_loss,\n",
    "        'acc': accuracy_score(y_true, y_pred),\n",
    "        'f1m': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'f1w': f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'recm': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'cm': confusion_matrix(y_true, y_pred)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "##############################################################################################################\n",
    "# ==============================================================================\n",
    "# CELDA 3: train_student (Actualizada)\n",
    "# ==============================================================================\n",
    "# (AquÃ­ se asume que las clases LabeledSpectro, EarlyStopping, etc. de tu celda 3 original ya estÃ¡n definidas)\n",
    "\n",
    "def train_student(params, train_loader, val_loader, num_classes, class_names):\n",
    "    device = torch.device(config.DEVICE)\n",
    "    torch.manual_seed(config.SEED)\n",
    "    np.random.seed(config.SEED)\n",
    "\n",
    "    # Inicializar el Asistente de Tesis\n",
    "    helper = ThesisHelper(params, class_names, base_dir=config.RESULTS_DIR, run_type='student')\n",
    "\n",
    "    model = models.make_model(\n",
    "        params['MODEL_NAME'], \n",
    "        num_classes, \n",
    "        params.get('RESNET_USE_PRETRAIN', True)\n",
    "    ).to(device)\n",
    "    \n",
    "    opt = torch.optim.SGD(model.parameters(), lr=params['LR'], momentum=params['MOMENTUM'], weight_decay=params['WEIGHT_DECAY'])\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=params['EPOCHS'], eta_min=params['LR'] * config.ETA_MIN_FACTOR)\n",
    "    \n",
    "    es = EarlyStopping(patience=params['PATIENCE'], min_delta=config.MIN_DELTA, restore_best=False) # restore_best=False, ThesisHelper se encarga\n",
    "    t0 = time.time()\n",
    "\n",
    "    for ep in range(1, params['EPOCHS'] + 1):\n",
    "        model.train()\n",
    "        tr_loss, n = 0.0, 0\n",
    "        tr_preds, tr_gts = [], [] # Para calcular tr_acc\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb = maybe_resize_for_resnet(xb, params.get('RESNET_RESIZE_TO_224', False))\n",
    "            xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "            \n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=config.CLIP_MAX_NORM)\n",
    "            opt.step()\n",
    "            \n",
    "            tr_loss += loss.item() * xb.size(0)\n",
    "            n += xb.size(0)\n",
    "            \n",
    "            # Guardar predicciones y etiquetas para tr_acc\n",
    "            tr_preds.append(logits.softmax(1).argmax(1).cpu())\n",
    "            tr_gts.append(yb.cpu())\n",
    "        \n",
    "        tr_loss /= n\n",
    "        sched.step()\n",
    "\n",
    "        # Calcular tr_acc\n",
    "        y_pred_tr = torch.cat(tr_preds).numpy()\n",
    "        y_true_tr = torch.cat(tr_gts).numpy()\n",
    "        tr_acc = accuracy_score(y_true_tr, y_pred_tr)\n",
    "\n",
    "        # Evaluar en validaciÃ³n\n",
    "        val_metrics = evaluate(model, val_loader, crit, params)\n",
    "        monitor_metric_key = config.MONITOR.replace('_macro', 'm').replace('_weighted', 'w')\n",
    "        monitor_metric_val = val_metrics[monitor_metric_key]\n",
    "        \n",
    "        # Consolidar todas las mÃ©tricas de la Ã©poca\n",
    "        log_entry = {\n",
    "            'epoch': ep, 'tr_loss': tr_loss, 'tr_acc': tr_acc, \n",
    "            **val_metrics, 'lr': sched.get_last_lr()[0]\n",
    "        }\n",
    "        \n",
    "        # Usar ThesisHelper para registrar y guardar el mejor modelo\n",
    "        helper.log_epoch(model, log_entry)\n",
    "        \n",
    "        print(f\"[{params['MODEL_NAME']}] e{ep:03d}/{params['EPOCHS']} | tr_loss={tr_loss:.4f} | va_loss={val_metrics['loss']:.4f} | tr_acc={tr_acc:.4f} | va_acc={val_metrics['acc']:.4f} | {config.MONITOR}={monitor_metric_val:.4f}\")\n",
    "\n",
    "        # Comprobar Early Stopping\n",
    "        stop = es.step(monitor_metric_val, model)\n",
    "        if stop:\n",
    "            print(f\"Early stopping en epoch {ep} (mejor {config.MONITOR}={es.best:.4f}).\")\n",
    "            break\n",
    "\n",
    "    dur = time.time() - t0\n",
    "    \n",
    "    # Finalizar el proceso: guardar resÃºmenes, grÃ¡ficas, etc.\n",
    "    helper.finalize(dur)\n",
    "    \n",
    "    # Cargar el mejor modelo guardado por el helper para devolverlo\n",
    "    best_model_path = helper.output_dir / 'best_model.pth'\n",
    "    if best_model_path.exists():\n",
    "        model.load_state_dict(torch.load(best_model_path))\n",
    "        print(f\"Modelo final cargado desde el mejor checkpoint (F1-Macro: {helper.best_f1_macro:.4f}).\")\n",
    "\n",
    "    print(f\"[DONE] {params['MODEL_NAME']} | dur={dur/60:.1f} min | best_{config.MONITOR}={helper.best_f1_macro:.4f}\")\n",
    "    \n",
    "    return {'model': model, 'helper': helper}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3795b078-13f5-400a-94b9-fdd735caec9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando un Student con la arquitectura 'resnet50'...\n",
      "Usando pseudo-etiquetas verificadas de modelo Teacher ganador...\n",
      "  - Cargando desde 'resnet50'...\n",
      "\n",
      "Dataset de Entrenamiento Original: 1049 muestras\n",
      "Pseudo-Etiquetas Ãšnicas AÃ±adidas: 2416 muestras\n",
      "Dataset de Entrenamiento Aumentado Total: 3465 muestras\n",
      "Dataset de ValidaciÃ³n: 263 muestras\n",
      "\n",
      "ThesisHelper inicializado para 'student_trained_with_win_teachersresnet50'. Artefactos se guardarÃ¡n en: D:\\PYTHON\\30_CLASIFICADOR_DE_INTERFERENCIAS\\RESULTADOS\\Carrier_C4_9435\\student_trained_with_win_teachersresnet50\n",
      "ðŸš€ Nuevo mejor F1-Macro: 0.5700 en la Ã©poca 1. Guardando checkpoint...\n",
      "[resnet50] e001/300 | tr_loss=0.7010 | va_loss=0.6124 | tr_acc=0.7642 | va_acc=0.7985 | f1_macro=0.5700\n",
      "ðŸš€ Nuevo mejor F1-Macro: 0.7540 en la Ã©poca 2. Guardando checkpoint...\n",
      "[resnet50] e002/300 | tr_loss=0.2090 | va_loss=0.3635 | tr_acc=0.9354 | va_acc=0.8897 | f1_macro=0.7540\n",
      "ðŸš€ Nuevo mejor F1-Macro: 0.8991 en la Ã©poca 3. Guardando checkpoint...\n",
      "[resnet50] e003/300 | tr_loss=0.1087 | va_loss=0.1544 | tr_acc=0.9680 | va_acc=0.9506 | f1_macro=0.8991\n",
      "[resnet50] e004/300 | tr_loss=0.0890 | va_loss=0.3550 | tr_acc=0.9703 | va_acc=0.8745 | f1_macro=0.7889\n",
      "[resnet50] e005/300 | tr_loss=0.0648 | va_loss=0.1567 | tr_acc=0.9795 | va_acc=0.9316 | f1_macro=0.8794\n",
      "[resnet50] e006/300 | tr_loss=0.0543 | va_loss=0.1501 | tr_acc=0.9824 | va_acc=0.9430 | f1_macro=0.8838\n",
      "[resnet50] e007/300 | tr_loss=0.0455 | va_loss=0.2267 | tr_acc=0.9856 | va_acc=0.9316 | f1_macro=0.8789\n",
      "[resnet50] e008/300 | tr_loss=0.0330 | va_loss=0.2151 | tr_acc=0.9916 | va_acc=0.9278 | f1_macro=0.8697\n",
      "ðŸš€ Nuevo mejor F1-Macro: 0.9247 en la Ã©poca 9. Guardando checkpoint...\n",
      "[resnet50] e009/300 | tr_loss=0.0304 | va_loss=0.1387 | tr_acc=0.9911 | va_acc=0.9620 | f1_macro=0.9247\n",
      "ðŸš€ Nuevo mejor F1-Macro: 0.9440 en la Ã©poca 10. Guardando checkpoint...\n",
      "[resnet50] e010/300 | tr_loss=0.0299 | va_loss=0.0751 | tr_acc=0.9908 | va_acc=0.9696 | f1_macro=0.9440\n",
      "[resnet50] e011/300 | tr_loss=0.0209 | va_loss=0.0903 | tr_acc=0.9937 | va_acc=0.9734 | f1_macro=0.9348\n",
      "[resnet50] e012/300 | tr_loss=0.0185 | va_loss=0.1184 | tr_acc=0.9954 | va_acc=0.9620 | f1_macro=0.9255\n",
      "ðŸš€ Nuevo mejor F1-Macro: 0.9463 en la Ã©poca 13. Guardando checkpoint...\n",
      "[resnet50] e013/300 | tr_loss=0.0178 | va_loss=0.1134 | tr_acc=0.9965 | va_acc=0.9696 | f1_macro=0.9463\n",
      "[resnet50] e014/300 | tr_loss=0.0211 | va_loss=0.1501 | tr_acc=0.9939 | va_acc=0.9658 | f1_macro=0.9401\n",
      "[resnet50] e015/300 | tr_loss=0.0178 | va_loss=0.1030 | tr_acc=0.9945 | va_acc=0.9696 | f1_macro=0.9443\n",
      "[resnet50] e016/300 | tr_loss=0.0144 | va_loss=0.1075 | tr_acc=0.9957 | va_acc=0.9696 | f1_macro=0.9443\n",
      "[resnet50] e017/300 | tr_loss=0.0170 | va_loss=0.1205 | tr_acc=0.9954 | va_acc=0.9582 | f1_macro=0.9123\n",
      "ðŸš€ Nuevo mejor F1-Macro: 0.9609 en la Ã©poca 18. Guardando checkpoint...\n",
      "[resnet50] e018/300 | tr_loss=0.0165 | va_loss=0.0898 | tr_acc=0.9954 | va_acc=0.9772 | f1_macro=0.9609\n",
      "[resnet50] e019/300 | tr_loss=0.0105 | va_loss=0.1888 | tr_acc=0.9971 | va_acc=0.9506 | f1_macro=0.9233\n",
      "[resnet50] e020/300 | tr_loss=0.0115 | va_loss=0.1158 | tr_acc=0.9965 | va_acc=0.9696 | f1_macro=0.9538\n",
      "[resnet50] e021/300 | tr_loss=0.0102 | va_loss=0.1375 | tr_acc=0.9974 | va_acc=0.9696 | f1_macro=0.9609\n",
      "ðŸš€ Nuevo mejor F1-Macro: 0.9609 en la Ã©poca 22. Guardando checkpoint...\n",
      "[resnet50] e022/300 | tr_loss=0.0070 | va_loss=0.1150 | tr_acc=0.9983 | va_acc=0.9772 | f1_macro=0.9609\n",
      "[resnet50] e023/300 | tr_loss=0.0106 | va_loss=0.1401 | tr_acc=0.9977 | va_acc=0.9620 | f1_macro=0.9338\n",
      "[resnet50] e024/300 | tr_loss=0.0053 | va_loss=0.1323 | tr_acc=0.9991 | va_acc=0.9658 | f1_macro=0.9511\n",
      "[resnet50] e025/300 | tr_loss=0.0063 | va_loss=0.1215 | tr_acc=0.9986 | va_acc=0.9734 | f1_macro=0.9583\n",
      "[resnet50] e026/300 | tr_loss=0.0077 | va_loss=0.0904 | tr_acc=0.9983 | va_acc=0.9734 | f1_macro=0.9583\n",
      "[resnet50] e027/300 | tr_loss=0.0079 | va_loss=0.1299 | tr_acc=0.9977 | va_acc=0.9734 | f1_macro=0.9583\n",
      "[resnet50] e028/300 | tr_loss=0.0070 | va_loss=0.1367 | tr_acc=0.9977 | va_acc=0.9734 | f1_macro=0.9499\n",
      "[resnet50] e029/300 | tr_loss=0.0060 | va_loss=0.1480 | tr_acc=0.9980 | va_acc=0.9696 | f1_macro=0.9487\n",
      "[resnet50] e030/300 | tr_loss=0.0072 | va_loss=0.1732 | tr_acc=0.9986 | va_acc=0.9696 | f1_macro=0.9538\n",
      "[resnet50] e031/300 | tr_loss=0.0066 | va_loss=0.1453 | tr_acc=0.9986 | va_acc=0.9658 | f1_macro=0.9513\n",
      "[resnet50] e032/300 | tr_loss=0.0080 | va_loss=0.1419 | tr_acc=0.9971 | va_acc=0.9696 | f1_macro=0.9443\n",
      "[resnet50] e033/300 | tr_loss=0.0032 | va_loss=0.1315 | tr_acc=0.9991 | va_acc=0.9734 | f1_macro=0.9584\n",
      "Early stopping en epoch 33 (mejor f1_macro=0.9609).\n",
      "ðŸ“„ Historial y resumen guardados.\n",
      "ðŸ“Š GrÃ¡ficas de entrenamiento guardadas.\n",
      "ðŸ“‹ Tabla LaTeX generada.\n",
      "âœ… MÃ©tricas guardadas en: D:\\PYTHON\\30_CLASIFICADOR_DE_INTERFERENCIAS\\RESULTADOS\\metrics_por_carrier.xlsx\n",
      "âœ… MÃ©tricas finales registradas en Excel.\n",
      "Modelo final cargado desde el mejor checkpoint (F1-Macro: 0.9609).\n",
      "[DONE] resnet50 | dur=13.7 min | best_f1_macro=0.9609\n",
      "\n",
      "--- Proceso de Entrenamiento Completado ---\n",
      "Puedes encontrar todos los artefactos en la carpeta de resultados.\n",
      "\n",
      "Tabla LaTeX generada para tu informe:\n",
      "\n",
      "\\begin{table}[h!]\n",
      "\\centering\n",
      "\\caption{Resumen del entrenamiento del modelo student trained with win teachersresnet50 y mÃ©tricas finales en la mejor Ã©poca.}\n",
      "\\label{tab:training_summary_student_trained_with_win_teachersresnet50}\n",
      "\\begin{tabular}{ll}\n",
      "\\hline\n",
      "\\textbf{ParÃ¡metro} & \\textbf{Valor} \\\\\n",
      "\\hline\n",
      "Modelo Base & resnet50 \\\\\n",
      "Mejor Ã‰poca & 22 \\\\\n",
      "DuraciÃ³n Total (min) & 13.71 \\\\\n",
      "\\hline\n",
      "\\textbf{MÃ©trica de ValidaciÃ³n} & \\textbf{Valor} \\\\\n",
      "\\hline\n",
      "F1-Macro (Mejor) & 0.9609 \\\\\n",
      "Exactitud (Accuracy) & 0.9772 \\\\\n",
      "PÃ©rdida (Loss) & 0.1150 \\\\\n",
      "Recall (Macro) & 0.9586 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jorge Zegarra\\AppData\\Local\\Temp\\ipykernel_1292\\474062252.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELDA 4: Bucle Principal de Entrenamiento del Student (Actualizada)\n",
    "# ==============================================================================\n",
    "\n",
    "# --- CONFIGURACIÃ“N PARA ESTA EJECUCIÃ“N ---\n",
    "STUDENT_MODEL_KEY = 'resnet50' \n",
    "\n",
    "print(f\"Entrenando un Student con la arquitectura '{STUDENT_MODEL_KEY}'...\")\n",
    "print(f\"Usando pseudo-etiquetas verificadas de modelo Teacher ganador...\")\n",
    "\n",
    "# --- Carga de datos AUMENTADA ---\n",
    "class_names = sorted([p.name for p in config.TRAIN_VAL_DIR.iterdir() if p.is_dir()])\n",
    "cls2idx = {name: i for i, name in enumerate(class_names)}\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# 1. Cargar datos de entrenamiento originales (split train/val)\n",
    "all_original_files, all_original_labels = [], []\n",
    "for class_name in class_names:\n",
    "    class_path = config.TRAIN_VAL_DIR / class_name\n",
    "    files = list(class_path.glob(\"*.png\"))\n",
    "    all_original_files.extend(files)\n",
    "    all_original_labels.extend([cls2idx[class_name]] * len(files))\n",
    "\n",
    "original_train_files, val_files, original_train_labels, val_labels = train_test_split(\n",
    "    all_original_files, all_original_labels, test_size=0.2, random_state=config.SEED, stratify=all_original_labels\n",
    ")\n",
    "\n",
    "# 2. Cargar datos de pseudo-etiquetas de TODOS los teachers\n",
    "all_pseudo_files = set()\n",
    "for model_key in config.TRAIN_PARAMS.keys():\n",
    "    pseudo_root = config.PSEUDO_DIR / model_key\n",
    "    if pseudo_root.exists():\n",
    "        print(f\"  - Cargando desde '{model_key}'...\")\n",
    "        for class_path in pseudo_root.glob('*'):\n",
    "             if class_path.is_dir():\n",
    "                for file_path in class_path.glob(\"*.png\"):\n",
    "                    all_pseudo_files.add(file_path)\n",
    "\n",
    "pseudo_files = sorted(list(all_pseudo_files))\n",
    "pseudo_labels = [cls2idx[fp.parent.name] for fp in pseudo_files if fp.parent.name in cls2idx]\n",
    "pseudo_files = [fp for fp in pseudo_files if fp.parent.name in cls2idx] # Asegurarse de que coincidan\n",
    "\n",
    "# 3. Combinar datasets\n",
    "augmented_train_files = original_train_files + pseudo_files\n",
    "augmented_train_labels = original_train_labels + pseudo_labels\n",
    "\n",
    "# 4. Crear Datasets y Dataloaders\n",
    "train_ds = LabeledSpectro(augmented_train_files, augmented_train_labels, transform=weak_aug)\n",
    "val_ds = LabeledSpectro(val_files, val_labels, transform=None)\n",
    "\n",
    "print(f\"\\nDataset de Entrenamiento Original: {len(original_train_files)} muestras\")\n",
    "print(f\"Pseudo-Etiquetas Ãšnicas AÃ±adidas: {len(pseudo_files)} muestras\")\n",
    "print(f\"Dataset de Entrenamiento Aumentado Total: {len(train_ds)} muestras\")\n",
    "print(f\"Dataset de ValidaciÃ³n: {len(val_ds)} muestras\\n\")\n",
    "\n",
    "# --- Entrenamiento del Student ---\n",
    "params = config.TRAIN_PARAMS[STUDENT_MODEL_KEY]\n",
    "train_loader = DataLoader(train_ds, batch_size=params['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_ds, batch_size=params['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "\n",
    "results = train_student(params, train_loader, val_loader, num_classes, class_names)\n",
    "\n",
    "# --- ImpresiÃ³n final ---\n",
    "print(\"\\n--- Proceso de Entrenamiento Completado ---\")\n",
    "print(\"Puedes encontrar todos los artefactos en la carpeta de resultados.\")\n",
    "# Opcional: imprimir la tabla LaTeX directamente en la salida\n",
    "with open(results['helper'].output_dir / 'summary_table.tex', 'r') as f:\n",
    "    print(\"\\nTabla LaTeX generada para tu informe:\")\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b23393-7552-4b3b-9371-d3ed0e1dc865",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
