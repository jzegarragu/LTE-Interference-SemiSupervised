{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELDA 1: Markdown de IntroducciÃ³n\n",
    "# ==============================================================================\n",
    "# ## ETAPA 1: Entrenamiento de los \"Teachers\"\n",
    "#\n",
    "# Este notebook entrena los modelos iniciales (`cnn_paper`, `cnn_paper_L2`, `resnet50`) \n",
    "# utilizando Ãºnicamente los datos del subconjunto `train_val` que fue creado \n",
    "# por el script `00_prepare_data.py`.\n",
    "#\n",
    "# El objetivo es generar modelos base competentes que luego actuarÃ¡n como \n",
    "# \"maestros\" para etiquetar los datos no clasificados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELDA 2: Importaciones\n",
    "# ==============================================================================\n",
    "# --- Importaciones --- \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix\n",
    "import time\n",
    "from pathlib import Path\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# --- Importar desde nuestros mÃ³dulos locales ---\n",
    "import config\n",
    "import models\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELDA 2.1: Clases y Funciones Auxiliares\n",
    "# ==============================================================================\n",
    "\n",
    "class ThesisHelper:\n",
    "    \"\"\"\n",
    "    Una clase para gestionar el logging, guardado de checkpoints y generaciÃ³n\n",
    "    de artefactos para una tesis durante el entrenamiento de un modelo.\n",
    "    \"\"\"\n",
    "    def __init__(self, params, class_names, base_dir, run_type='teacher'):\n",
    "        self.params = params\n",
    "        self.class_names = class_names\n",
    "        \n",
    "        # 1. Extraemos los hiperparÃ¡metros que queremos en el nombre\n",
    "        lr = self.params['LR']\n",
    "        wd = self.params['WEIGHT_DECAY']\n",
    "        \n",
    "        # 2. Creamos una \"etiqueta\" con estos valores\n",
    "        hparams_tag = f\"_lr{lr}_wd{wd}\"\n",
    "        \n",
    "        # 3. Construimos el nombre base como antes\n",
    "        if run_type == 'student':\n",
    "            base_name = f\"student_trained_with_win_teachers{self.params['MODEL_NAME']}\"\n",
    "        else: # Para los 'teacher'\n",
    "            base_name = f\"{run_type}_{self.params['MODEL_NAME']}\"\n",
    "            \n",
    "        # 4. Unimos el nombre base con la etiqueta de hiperparÃ¡metros\n",
    "        self.run_name = base_name + hparams_tag\n",
    "\n",
    "\n",
    "        self.output_dir = Path(base_dir) / self.run_name\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.run_type = run_type\n",
    "        \n",
    "        self.history = []\n",
    "        self.best_f1_macro = -1.0\n",
    "        self.best_epoch_metrics = None\n",
    "        \n",
    "        print(f\"ThesisHelper inicializado para '{self.run_name}'. Artefactos se guardarÃ¡n en: {self.output_dir}\")\n",
    "\n",
    "    def log_epoch(self, model, metrics):\n",
    "        self.history.append(metrics)\n",
    "        current_f1_macro = metrics['f1m']\n",
    "        \n",
    "        if current_f1_macro > self.best_f1_macro:\n",
    "            self.best_f1_macro = current_f1_macro\n",
    "            self.best_epoch_metrics = metrics\n",
    "            print(f\"ðŸš€ Nuevo mejor F1-Macro: {self.best_f1_macro:.4f} en la Ã©poca {metrics['epoch']}. Guardando checkpoint...\")\n",
    "            self._save_checkpoint(model)\n",
    "\n",
    "    def _save_checkpoint(self, model):\n",
    "        torch.save(model.state_dict(), self.output_dir / 'best_model.pth')\n",
    "\n",
    "    def finalize(self, total_duration_seconds):\n",
    "        if not self.history:\n",
    "            print(\"No hay historial para finalizar. Saltando la generaciÃ³n de artefactos.\")\n",
    "            return\n",
    "\n",
    "        history_df = pd.DataFrame(self.history)\n",
    "        history_df.to_csv(self.output_dir / 'training_history.csv', index=False)\n",
    "        \n",
    "        summary = self.best_epoch_metrics.copy()\n",
    "        summary['total_duration_min'] = total_duration_seconds / 60\n",
    "        cm = summary.pop('cm', None) \n",
    "        \n",
    "        with open(self.output_dir / 'summary.json', 'w') as f:\n",
    "            json.dump(summary, f, indent=4)\n",
    "        print(f\"ðŸ“„ Historial y resumen guardados.\")\n",
    "\n",
    "        self._plot_curves(history_df)\n",
    "        print(f\"ðŸ“Š GrÃ¡ficas de entrenamiento guardadas.\")\n",
    "        \n",
    "        self._generate_latex_table(summary)\n",
    "        print(f\"ðŸ“‹ Tabla LaTeX generada.\")\n",
    "        \n",
    "        self._log_to_excel(summary, cm)\n",
    "        print(f\"âœ… MÃ©tricas finales registradas en Excel.\")\n",
    "\n",
    "    def _plot_curves(self, df):\n",
    "        best_epoch = self.best_epoch_metrics['epoch']\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "        fig.suptitle(f'Curvas de Entrenamiento para {self.run_name}', fontsize=16)\n",
    "\n",
    "        ax1.plot(df['epoch'], df['tr_loss'], 'o-', label='Training Loss')\n",
    "        ax1.plot(df['epoch'], df['loss'], 'o-', label='Validation Loss')\n",
    "        ax1.axvline(x=best_epoch, color='r', linestyle='--', label=f'Mejor Ã‰poca ({best_epoch})')\n",
    "        ax1.set_ylabel('PÃ©rdida (Loss)')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "        \n",
    "        best_loss = self.best_epoch_metrics['loss']\n",
    "        ax1.annotate(f'Mejor F1-Macro\\nVal Loss: {best_loss:.4f}',\n",
    "                     xy=(best_epoch, best_loss),\n",
    "                     xytext=(best_epoch + 3, best_loss + 0.1*df['loss'].max()),\n",
    "                     arrowprops=dict(facecolor='black', shrink=0.05, width=1, headwidth=8),\n",
    "                     bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"yellow\", ec=\"black\", lw=1, alpha=0.7))\n",
    "\n",
    "        ax2.plot(df['epoch'], df['tr_acc'], 'o-', label='Training Accuracy')\n",
    "        ax2.plot(df['epoch'], df['acc'], 'o-', label='Validation Accuracy')\n",
    "        ax2.plot(df['epoch'], df['f1m'], 'o-', label='Validation F1-Macro', linewidth=2, markersize=8)\n",
    "        ax2.axvline(x=best_epoch, color='r', linestyle='--')\n",
    "        ax2.set_xlabel('Ã‰poca')\n",
    "        ax2.set_ylabel('MÃ©trica')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, linestyle='--', alpha=0.6)\n",
    "        ax2.yaxis.set_major_formatter(mticker.PercentFormatter(xmax=1.0))\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.savefig(self.output_dir / 'training_curves.png', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def _generate_latex_table(self, summary):\n",
    "        latex_str = f\"\"\"\n",
    "\\\\begin{{table}}[h!]\n",
    "\\\\centering\n",
    "\\\\caption{{Resumen del entrenamiento del modelo {self.run_name.replace('_', ' ')} y mÃ©tricas finales en la mejor Ã©poca.}}\n",
    "\\\\label{{tab:training_summary_{self.run_name}}}\n",
    "\\\\begin{{tabular}}{{ll}}\n",
    "\\\\hline\n",
    "\\\\textbf{{ParÃ¡metro}} & \\\\textbf{{Valor}} \\\\\\\\\n",
    "\\\\hline\n",
    "Modelo Base & {self.params['MODEL_NAME']} \\\\\\\\\n",
    "Mejor Ã‰poca & {summary['epoch']} \\\\\\\\\n",
    "DuraciÃ³n Total (min) & {summary['total_duration_min']:.2f} \\\\\\\\\n",
    "\\\\hline\n",
    "\\\\textbf{{MÃ©trica de ValidaciÃ³n}} & \\\\textbf{{Valor}} \\\\\\\\\n",
    "\\\\hline\n",
    "F1-Macro (Mejor) & {summary['f1m']:.4f} \\\\\\\\\n",
    "Exactitud (Accuracy) & {summary['acc']:.4f} \\\\\\\\\n",
    "PÃ©rdida (Loss) & {summary['loss']:.4f} \\\\\\\\\n",
    "Recall (Macro) & {summary['recm']:.4f} \\\\\\\\\n",
    "\\\\hline\n",
    "\\\\end{{tabular}}\n",
    "\\\\end{{table}}\n",
    "        \"\"\"\n",
    "        with open(self.output_dir / 'summary_table.tex', 'w') as f:\n",
    "            f.write(latex_str)\n",
    "\n",
    "    def _log_to_excel(self, summary, cm):\n",
    "        note = f\"{self.run_type.capitalize()} - Mejor checkpoint en la Ã©poca {summary['epoch']}\"\n",
    "        \n",
    "        metrics_to_log = {\n",
    "            'carrier': config.CARRIER,\n",
    "            'model_name': self.params['MODEL_NAME'],\n",
    "            'run_tag': self.run_name,\n",
    "            'num_classes': len(self.class_names),\n",
    "            'acc': summary['acc'],\n",
    "            'loss': summary['loss'],\n",
    "            'f1m': summary['f1m'],\n",
    "            'f1w': summary['f1w'],\n",
    "            'recm': summary['recm'],\n",
    "            'cm': cm,\n",
    "            'epochs': self.params['EPOCHS'],\n",
    "            'batch_size': self.params['BATCH_SIZE'],\n",
    "            'lr': self.params['LR'],\n",
    "            'weight_decay': self.params['WEIGHT_DECAY'],\n",
    "            'notes': note\n",
    "        }\n",
    "        utils.log_metrics_excel(config.EXCEL_PATH, config.RESULTS_DIR, self.class_names, metrics_to_log)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "    # CELDA 2.2: Clases de Dataset y Aumentos de Datos\n",
    "# ==============================================================================\n",
    "# ### Clases de Dataset y Aumentos de Datos\n",
    "# Definimos las clases para manejar los datasets y las transformaciones de aumento de datos.\n",
    "\n",
    "class RandomTimeShift(torch.nn.Module):\n",
    "    def __init__(self, max_frac=0.1):\n",
    "        super().__init__()\n",
    "        self.max_frac = max_frac\n",
    "    def forward(self, x):\n",
    "        _, H, W = x.shape\n",
    "        s = int(random.uniform(-self.max_frac, self.max_frac) * W)\n",
    "        return torch.roll(x, shifts=s, dims=-1)\n",
    "\n",
    "class RandomGain(torch.nn.Module):\n",
    "    def __init__(self, a=0.95, b=1.05):\n",
    "        super().__init__()\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "    def forward(self, x):\n",
    "        g = random.uniform(self.a, self.b)\n",
    "        return (x * g).clamp(0, 1)\n",
    "\n",
    "weak_aug = T.Compose([\n",
    "    RandomTimeShift(0.08),\n",
    "    RandomGain(0.95, 1.05),\n",
    "])\n",
    "\n",
    "class LabeledSpectro(Dataset):\n",
    "    def __init__(self, files, labels, transform=None):\n",
    "        self.files = files\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    def __getitem__(self, i):\n",
    "        path = self.files[i]\n",
    "        try:\n",
    "            x = utils.load_png_gray(path)\n",
    "            if self.transform:\n",
    "                x = self.transform(x)\n",
    "            y = self.labels[i]\n",
    "            return x, y\n",
    "        except Exception as e:\n",
    "            print(f\"[dataset_warning] Saltando archivo por error: {e}\")\n",
    "            # Devuelve un tensor vacÃ­o y la etiqueta para no romper el batch\n",
    "            return torch.zeros(1, config.IMG_H, config.IMG_W, dtype=torch.float32), self.labels[i]\n",
    "\n",
    "def maybe_resize_for_resnet(x, should_resize):\n",
    "    if should_resize:\n",
    "        return torch.nn.functional.interpolate(x, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "    return x\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# CELDA 2.3: LÃ³gica de Entrenamiento y EvaluaciÃ³n\n",
    "# ==============================================================================\n",
    "# ### LÃ³gica de Entrenamiento y EvaluaciÃ³n\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience, min_delta, mode='max', restore_best=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.restore_best = restore_best\n",
    "        self.best = -float('inf') if mode == 'max' else float('inf')\n",
    "        self.wait = 0\n",
    "        self.best_state = None\n",
    "\n",
    "    def step(self, metric, model):\n",
    "        is_better = (metric > self.best + self.min_delta) if self.mode == 'max' else (metric < self.best - self.min_delta)\n",
    "        if is_better:\n",
    "            self.best = metric\n",
    "            self.wait = 0\n",
    "            if self.restore_best:\n",
    "                self.best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            return False # No detener\n",
    "        self.wait += 1\n",
    "        return self.wait >= self.patience # Detener\n",
    "\n",
    "    def restore(self, model):\n",
    "        if self.restore_best and self.best_state is not None:\n",
    "            model.load_state_dict(self.best_state)\n",
    "\n",
    "def evaluate(model, loader, criterion, params):\n",
    "    device = torch.device(config.DEVICE)\n",
    "    model.eval()\n",
    "    va_loss, preds, gts = 0.0, [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = maybe_resize_for_resnet(xb, params.get('RESNET_RESIZE_TO_224', False))\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            va_loss += loss.item() * xb.size(0)\n",
    "            preds.append(logits.softmax(1).argmax(1).cpu())\n",
    "            gts.append(yb.cpu())\n",
    "    va_loss /= len(loader.dataset)\n",
    "    y_pred = torch.cat(preds).numpy()\n",
    "    y_true = torch.cat(gts).numpy()\n",
    "    \n",
    "    metrics = {\n",
    "        'loss': va_loss,\n",
    "        'acc': accuracy_score(y_true, y_pred),\n",
    "        'f1m': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'f1w': f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'recm': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'cm': confusion_matrix(y_true, y_pred)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# (AquÃ­ van el resto de clases y funciones: RandomTimeShift, RandomGain, LabeledSpectro, maybe_resize, EarlyStopping, evaluate)\n",
    "\n",
    "def run_training_session(params, train_loader, val_loader, num_classes, class_names, run_type):\n",
    "    device = torch.device(config.DEVICE)\n",
    "    torch.manual_seed(config.SEED)\n",
    "    np.random.seed(config.SEED)\n",
    "    helper = ThesisHelper(params, class_names, base_dir=config.RESULTS_DIR, run_type=run_type)\n",
    "    model = models.make_model(\n",
    "        params['MODEL_NAME'], \n",
    "        num_classes, \n",
    "        params.get('RESNET_USE_PRETRAIN', True)\n",
    "    ).to(device)\n",
    "    opt = torch.optim.SGD(model.parameters(), lr=params['LR'], momentum=params['MOMENTUM'], weight_decay=params['WEIGHT_DECAY'])\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=params['EPOCHS'], eta_min=params['LR'] * config.ETA_MIN_FACTOR)\n",
    "    es = EarlyStopping(patience=params['PATIENCE'], min_delta=config.MIN_DELTA, restore_best=False)\n",
    "    t0 = time.time()\n",
    "    for ep in range(1, params['EPOCHS'] + 1):\n",
    "        model.train()\n",
    "        tr_loss, n = 0.0, 0\n",
    "        tr_preds, tr_gts = [], []\n",
    "        for xb, yb in train_loader:\n",
    "            xb = maybe_resize_for_resnet(xb, params.get('RESNET_RESIZE_TO_224', False))\n",
    "            xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=config.CLIP_MAX_NORM)\n",
    "            opt.step()\n",
    "            tr_loss += loss.item() * xb.size(0)\n",
    "            n += xb.size(0)\n",
    "            tr_preds.append(logits.softmax(1).argmax(1).cpu())\n",
    "            tr_gts.append(yb.cpu())\n",
    "        tr_loss /= n\n",
    "        sched.step()\n",
    "        y_pred_tr = torch.cat(tr_preds).numpy()\n",
    "        y_true_tr = torch.cat(tr_gts).numpy()\n",
    "        tr_acc = accuracy_score(y_true_tr, y_pred_tr)\n",
    "        val_metrics = evaluate(model, val_loader, crit, params)\n",
    "        monitor_metric_key = config.MONITOR.replace('_macro', 'm').replace('_weighted', 'w')\n",
    "        monitor_metric_val = val_metrics[monitor_metric_key]\n",
    "        log_entry = {\n",
    "            'epoch': ep, 'tr_loss': tr_loss, 'tr_acc': tr_acc, \n",
    "            **val_metrics, 'lr': sched.get_last_lr()[0]\n",
    "        }\n",
    "        helper.log_epoch(model, log_entry)\n",
    "        print(f\"[{params['MODEL_NAME']}] e{ep:03d}/{params['EPOCHS']} | tr_loss={tr_loss:.4f} | va_loss={val_metrics['loss']:.4f} | tr_acc={tr_acc:.4f} | va_acc={val_metrics['acc']:.4f} | {config.MONITOR}={monitor_metric_val:.4f}\")\n",
    "        stop = es.step(monitor_metric_val, model)\n",
    "        if stop:\n",
    "            print(f\"Early stopping en epoch {ep} (mejor {config.MONITOR}={es.best:.4f}).\")\n",
    "            break\n",
    "    dur = time.time() - t0\n",
    "    helper.finalize(dur)\n",
    "    best_model_path = helper.output_dir / 'best_model.pth'\n",
    "    if best_model_path.exists():\n",
    "        model.load_state_dict(torch.load(best_model_path))\n",
    "        print(f\"Modelo final cargado desde el mejor checkpoint (F1-Macro: {helper.best_f1_macro:.4f}).\")\n",
    "    print(f\"[DONE] {params['MODEL_NAME']} | dur={dur/60:.1f} min | best_{config.MONITOR}={helper.best_f1_macro:.4f}\")\n",
    "    return {'model': model, 'helper': helper}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELDA 3: FunciÃ³n para Generar Artefactos de ComparaciÃ³n\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_teacher_comparison_artifacts(results_dir, teacher_keys):\n",
    "    \"\"\"\n",
    "    Carga los resÃºmenes de cada Teacher, y genera una grÃ¡fica de barras\n",
    "    y una tabla LaTeX para compararlos.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Generando Artefactos de ComparaciÃ³n de Teachers ---\")\n",
    "    summaries = []\n",
    "    for key in teacher_keys:\n",
    "        summary_path = Path(results_dir) / f\"teacher_{key}\" / \"summary.json\"\n",
    "        if summary_path.exists():\n",
    "            with open(summary_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                data['model'] = key\n",
    "                summaries.append(data)\n",
    "        else:\n",
    "            print(f\"[Advertencia] No se encontrÃ³ el resumen para el teacher: {key}\")\n",
    "\n",
    "    if not summaries:\n",
    "        print(\"No se encontraron resÃºmenes para generar la comparaciÃ³n.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(summaries).sort_values('f1m', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # --- 1. Generar GrÃ¡fica de Barras Comparativa ---\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    bars = ax.bar(df['model'], df['f1m'], color=['#ff7f0e' if i == 0 else '#1f77b4' for i in df.index])\n",
    "    ax.bar_label(bars, fmt='%.4f', padding=3)\n",
    "    ax.set_ylabel('F1-Score Macro (ValidaciÃ³n)')\n",
    "    ax.set_xlabel('Arquitectura del Modelo Teacher')\n",
    "    ax.set_title('ComparaciÃ³n de Rendimiento de Modelos Teacher', fontsize=16)\n",
    "    ax.set_ylim(0, max(df['f1m']) * 1.1)\n",
    "    fig.tight_layout()\n",
    "    plot_path = Path(results_dir) / \"comparison_teachers_performance.png\"\n",
    "    plt.savefig(plot_path, dpi=300)\n",
    "    print(f\"ðŸ“Š GrÃ¡fica de comparaciÃ³n guardada en: {plot_path}\")\n",
    "    plt.close()\n",
    "\n",
    "    # --- 2. Generar Tabla LaTeX Comparativa ---\n",
    "    df_latex = df[['model', 'f1m', 'acc', 'loss']].copy()\n",
    "    df_latex.rename(columns={'model': 'Modelo', 'f1m': 'F1-Macro', 'acc': 'Accuracy', 'loss': 'Loss'}, inplace=True)\n",
    "    \n",
    "    df_latex['Modelo'] = df_latex.apply(lambda row: f\"\\\\textbf{{{row.Modelo}}}\" if row.name == 0 else row.Modelo, axis=1)\n",
    "\n",
    "    latex_table = df_latex.to_latex(index=False, float_format=\"%.4f\", caption=\"ComparaciÃ³n de mÃ©tricas finales de validaciÃ³n para los modelos Teacher. El mejor modelo (seleccionado por F1-Macro) se muestra en negrita.\", label=\"tab:teacher_comparison\", position=\"h!\")\n",
    "    \n",
    "    table_path = Path(results_dir) / \"comparison_teachers_table.tex\"\n",
    "    with open(table_path, 'w') as f:\n",
    "        f.write(latex_table)\n",
    "    print(f\"ðŸ“‹ Tabla LaTeX de comparaciÃ³n guardada en: {table_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos listos: 1049 muestras de entrenamiento, 263 de validaciÃ³n.\n",
      "\n",
      "==================== ENTRENANDO TEACHER: CNN_PAPER ====================\n",
      "ThesisHelper inicializado para 'teacher_cnn_paper_lr0.001_wd0.0'. Artefactos se guardarÃ¡n en: D:\\PYTHON\\30_CLASIFICADOR_DE_INTERFERENCIAS\\RESULTADOS\\Carrier_C4_9435\\teacher_cnn_paper_lr0.001_wd0.0\n",
      "ðŸš€ Nuevo mejor F1-Macro: 0.7135 en la Ã©poca 1. Guardando checkpoint...\n",
      "[cnn_paper] e001/300 | tr_loss=0.9704 | va_loss=0.8247 | tr_acc=0.6702 | va_acc=0.8669 | f1_macro=0.7135\n",
      "ðŸš€ Nuevo mejor F1-Macro: 0.8484 en la Ã©poca 2. Guardando checkpoint...\n",
      "[cnn_paper] e002/300 | tr_loss=0.4973 | va_loss=0.3211 | tr_acc=0.8379 | va_acc=0.9125 | f1_macro=0.8484\n",
      "ðŸš€ Nuevo mejor F1-Macro: 0.8619 en la Ã©poca 3. Guardando checkpoint...\n",
      "[cnn_paper] e003/300 | tr_loss=0.3832 | va_loss=0.2312 | tr_acc=0.8723 | va_acc=0.9202 | f1_macro=0.8619\n",
      "ðŸš€ Nuevo mejor F1-Macro: 0.9196 en la Ã©poca 4. Guardando checkpoint...\n",
      "[cnn_paper] e004/300 | tr_loss=0.3221 | va_loss=0.2216 | tr_acc=0.8932 | va_acc=0.9468 | f1_macro=0.9196\n",
      "[cnn_paper] e005/300 | tr_loss=0.2781 | va_loss=0.1949 | tr_acc=0.9113 | va_acc=0.9430 | f1_macro=0.9037\n",
      "[cnn_paper] e006/300 | tr_loss=0.2462 | va_loss=0.2006 | tr_acc=0.9190 | va_acc=0.9316 | f1_macro=0.8921\n",
      "ðŸš€ Nuevo mejor F1-Macro: 0.9201 en la Ã©poca 7. Guardando checkpoint...\n",
      "[cnn_paper] e007/300 | tr_loss=0.2502 | va_loss=0.1733 | tr_acc=0.9066 | va_acc=0.9506 | f1_macro=0.9201\n",
      "[cnn_paper] e008/300 | tr_loss=0.2430 | va_loss=0.1970 | tr_acc=0.9161 | va_acc=0.9316 | f1_macro=0.8870\n",
      "[cnn_paper] e009/300 | tr_loss=0.2147 | va_loss=0.1920 | tr_acc=0.9276 | va_acc=0.9506 | f1_macro=0.9150\n",
      "[cnn_paper] e010/300 | tr_loss=0.2010 | va_loss=0.1912 | tr_acc=0.9266 | va_acc=0.9354 | f1_macro=0.8981\n",
      "[cnn_paper] e011/300 | tr_loss=0.1865 | va_loss=0.1599 | tr_acc=0.9399 | va_acc=0.9506 | f1_macro=0.9093\n",
      "[cnn_paper] e012/300 | tr_loss=0.1587 | va_loss=0.1613 | tr_acc=0.9504 | va_acc=0.9354 | f1_macro=0.9054\n",
      "[cnn_paper] e013/300 | tr_loss=0.1470 | va_loss=0.1681 | tr_acc=0.9514 | va_acc=0.9468 | f1_macro=0.9089\n",
      "ðŸš€ Nuevo mejor F1-Macro: 0.9208 en la Ã©poca 14. Guardando checkpoint...\n",
      "[cnn_paper] e014/300 | tr_loss=0.1327 | va_loss=0.1674 | tr_acc=0.9590 | va_acc=0.9544 | f1_macro=0.9208\n",
      "[cnn_paper] e015/300 | tr_loss=0.1249 | va_loss=0.1474 | tr_acc=0.9581 | va_acc=0.9544 | f1_macro=0.9133\n",
      "[cnn_paper] e016/300 | tr_loss=0.1217 | va_loss=0.1560 | tr_acc=0.9590 | va_acc=0.9468 | f1_macro=0.9015\n",
      "Early stopping en epoch 16 (mejor f1_macro=0.9196).\n",
      "ðŸ“„ Historial y resumen guardados.\n",
      "ðŸ“Š GrÃ¡ficas de entrenamiento guardadas.\n",
      "ðŸ“‹ Tabla LaTeX generada.\n",
      "âœ… MÃ©tricas guardadas en: D:\\PYTHON\\30_CLASIFICADOR_DE_INTERFERENCIAS\\RESULTADOS\\metrics_por_carrier.xlsx\n",
      "âœ… MÃ©tricas finales registradas en Excel.\n",
      "Modelo final cargado desde el mejor checkpoint (F1-Macro: 0.9208).\n",
      "[DONE] cnn_paper | dur=0.4 min | best_f1_macro=0.9208\n",
      "\n",
      "==================== ENTRENANDO TEACHER: CNN_PAPER_L2 ====================\n",
      "ThesisHelper inicializado para 'teacher_cnn_paper_L2_lr0.001_wd1e-05'. Artefactos se guardarÃ¡n en: D:\\PYTHON\\30_CLASIFICADOR_DE_INTERFERENCIAS\\RESULTADOS\\Carrier_C4_9435\\teacher_cnn_paper_L2_lr0.001_wd1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jorge Zegarra\\AppData\\Local\\Temp\\ipykernel_16376\\1181073326.py:332: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Nuevo mejor F1-Macro: 0.7104 en la Ã©poca 1. Guardando checkpoint...\n",
      "[cnn_paper_L2] e001/300 | tr_loss=0.9771 | va_loss=0.8476 | tr_acc=0.6606 | va_acc=0.8669 | f1_macro=0.7104\n",
      "ðŸš€ Nuevo mejor F1-Macro: 0.8823 en la Ã©poca 2. Guardando checkpoint...\n",
      "[cnn_paper_L2] e002/300 | tr_loss=0.4919 | va_loss=0.3053 | tr_acc=0.8456 | va_acc=0.9240 | f1_macro=0.8823\n",
      "ðŸš€ Nuevo mejor F1-Macro: 0.8854 en la Ã©poca 3. Guardando checkpoint...\n",
      "[cnn_paper_L2] e003/300 | tr_loss=0.3758 | va_loss=0.2287 | tr_acc=0.8751 | va_acc=0.9278 | f1_macro=0.8854\n",
      "ðŸš€ Nuevo mejor F1-Macro: 0.9085 en la Ã©poca 4. Guardando checkpoint...\n",
      "[cnn_paper_L2] e004/300 | tr_loss=0.3179 | va_loss=0.2283 | tr_acc=0.8875 | va_acc=0.9354 | f1_macro=0.9085\n",
      "[cnn_paper_L2] e005/300 | tr_loss=0.2898 | va_loss=0.1920 | tr_acc=0.9037 | va_acc=0.9430 | f1_macro=0.9015\n",
      "[cnn_paper_L2] e006/300 | tr_loss=0.2533 | va_loss=0.1869 | tr_acc=0.9247 | va_acc=0.9278 | f1_macro=0.8784\n",
      "ðŸš€ Nuevo mejor F1-Macro: 0.9099 en la Ã©poca 7. Guardando checkpoint...\n",
      "[cnn_paper_L2] e007/300 | tr_loss=0.2190 | va_loss=0.1742 | tr_acc=0.9218 | va_acc=0.9430 | f1_macro=0.9099\n",
      "[cnn_paper_L2] e008/300 | tr_loss=0.2433 | va_loss=0.1717 | tr_acc=0.9209 | va_acc=0.9392 | f1_macro=0.9082\n",
      "ðŸš€ Nuevo mejor F1-Macro: 0.9199 en la Ã©poca 9. Guardando checkpoint...\n",
      "[cnn_paper_L2] e009/300 | tr_loss=0.2355 | va_loss=0.1797 | tr_acc=0.9295 | va_acc=0.9506 | f1_macro=0.9199\n",
      "[cnn_paper_L2] e010/300 | tr_loss=0.1893 | va_loss=0.1867 | tr_acc=0.9399 | va_acc=0.9354 | f1_macro=0.9046\n",
      "[cnn_paper_L2] e011/300 | tr_loss=0.1761 | va_loss=0.1535 | tr_acc=0.9438 | va_acc=0.9506 | f1_macro=0.9171\n",
      "[cnn_paper_L2] e012/300 | tr_loss=0.1655 | va_loss=0.1605 | tr_acc=0.9418 | va_acc=0.9468 | f1_macro=0.9136\n",
      "[cnn_paper_L2] e013/300 | tr_loss=0.1705 | va_loss=0.1538 | tr_acc=0.9352 | va_acc=0.9468 | f1_macro=0.9003\n",
      "[cnn_paper_L2] e014/300 | tr_loss=0.1415 | va_loss=0.1697 | tr_acc=0.9523 | va_acc=0.9468 | f1_macro=0.9150\n",
      "[cnn_paper_L2] e015/300 | tr_loss=0.1275 | va_loss=0.1539 | tr_acc=0.9523 | va_acc=0.9506 | f1_macro=0.9164\n",
      "[cnn_paper_L2] e016/300 | tr_loss=0.1065 | va_loss=0.1553 | tr_acc=0.9685 | va_acc=0.9544 | f1_macro=0.9144\n",
      "[cnn_paper_L2] e017/300 | tr_loss=0.0994 | va_loss=0.1431 | tr_acc=0.9714 | va_acc=0.9544 | f1_macro=0.9186\n",
      "[cnn_paper_L2] e018/300 | tr_loss=0.0998 | va_loss=0.1508 | tr_acc=0.9685 | va_acc=0.9544 | f1_macro=0.9187\n",
      "[cnn_paper_L2] e019/300 | tr_loss=0.0994 | va_loss=0.2085 | tr_acc=0.9666 | va_acc=0.9354 | f1_macro=0.9000\n",
      "[cnn_paper_L2] e020/300 | tr_loss=0.0931 | va_loss=0.1598 | tr_acc=0.9733 | va_acc=0.9544 | f1_macro=0.9186\n",
      "[cnn_paper_L2] e021/300 | tr_loss=0.1090 | va_loss=0.1573 | tr_acc=0.9619 | va_acc=0.9506 | f1_macro=0.9174\n",
      "Early stopping en epoch 21 (mejor f1_macro=0.9199).\n",
      "ðŸ“„ Historial y resumen guardados.\n",
      "ðŸ“Š GrÃ¡ficas de entrenamiento guardadas.\n",
      "ðŸ“‹ Tabla LaTeX generada.\n",
      "âœ… MÃ©tricas guardadas en: D:\\PYTHON\\30_CLASIFICADOR_DE_INTERFERENCIAS\\RESULTADOS\\metrics_por_carrier.xlsx\n",
      "âœ… MÃ©tricas finales registradas en Excel.\n",
      "Modelo final cargado desde el mejor checkpoint (F1-Macro: 0.9199).\n",
      "[DONE] cnn_paper_L2 | dur=0.6 min | best_f1_macro=0.9199\n",
      "\n",
      "==================== ENTRENANDO TEACHER: RESNET50 ====================\n",
      "ThesisHelper inicializado para 'teacher_resnet50_lr0.001_wd1e-05'. Artefactos se guardarÃ¡n en: D:\\PYTHON\\30_CLASIFICADOR_DE_INTERFERENCIAS\\RESULTADOS\\Carrier_C4_9435\\teacher_resnet50_lr0.001_wd1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jorge Zegarra\\AppData\\Local\\Temp\\ipykernel_16376\\1181073326.py:332: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Nuevo mejor F1-Macro: 0.2606 en la Ã©poca 1. Guardando checkpoint...\n",
      "[resnet50] e001/300 | tr_loss=1.2979 | va_loss=1.4875 | tr_acc=0.5748 | va_acc=0.4297 | f1_macro=0.2606\n",
      "ðŸš€ Nuevo mejor F1-Macro: 0.6780 en la Ã©poca 2. Guardando checkpoint...\n",
      "[resnet50] e002/300 | tr_loss=0.5651 | va_loss=0.7158 | tr_acc=0.8265 | va_acc=0.8365 | f1_macro=0.6780\n",
      "ðŸš€ Nuevo mejor F1-Macro: 0.7429 en la Ã©poca 3. Guardando checkpoint...\n",
      "[resnet50] e003/300 | tr_loss=0.2938 | va_loss=0.5995 | tr_acc=0.9218 | va_acc=0.8631 | f1_macro=0.7429\n",
      "ðŸš€ Nuevo mejor F1-Macro: 0.7903 en la Ã©poca 4. Guardando checkpoint...\n",
      "[resnet50] e004/300 | tr_loss=0.1961 | va_loss=0.3797 | tr_acc=0.9476 | va_acc=0.8707 | f1_macro=0.7903\n",
      "ðŸš€ Nuevo mejor F1-Macro: 0.8669 en la Ã©poca 5. Guardando checkpoint...\n",
      "[resnet50] e005/300 | tr_loss=0.1377 | va_loss=0.2766 | tr_acc=0.9638 | va_acc=0.9163 | f1_macro=0.8669\n",
      "[resnet50] e006/300 | tr_loss=0.1262 | va_loss=0.6027 | tr_acc=0.9647 | va_acc=0.8137 | f1_macro=0.6867\n",
      "[resnet50] e007/300 | tr_loss=0.0763 | va_loss=0.2145 | tr_acc=0.9819 | va_acc=0.9049 | f1_macro=0.8392\n",
      "[resnet50] e008/300 | tr_loss=0.0511 | va_loss=0.3104 | tr_acc=0.9914 | va_acc=0.8935 | f1_macro=0.8542\n",
      "ðŸš€ Nuevo mejor F1-Macro: 0.9198 en la Ã©poca 9. Guardando checkpoint...\n",
      "[resnet50] e009/300 | tr_loss=0.0623 | va_loss=0.1888 | tr_acc=0.9819 | va_acc=0.9544 | f1_macro=0.9198\n",
      "ðŸš€ Nuevo mejor F1-Macro: 0.9288 en la Ã©poca 10. Guardando checkpoint...\n",
      "[resnet50] e010/300 | tr_loss=0.0435 | va_loss=0.1100 | tr_acc=0.9895 | va_acc=0.9658 | f1_macro=0.9288\n",
      "[resnet50] e011/300 | tr_loss=0.0332 | va_loss=0.2305 | tr_acc=0.9933 | va_acc=0.9278 | f1_macro=0.8590\n",
      "[resnet50] e012/300 | tr_loss=0.0332 | va_loss=0.3331 | tr_acc=0.9943 | va_acc=0.8897 | f1_macro=0.8492\n",
      "[resnet50] e013/300 | tr_loss=0.0301 | va_loss=0.3622 | tr_acc=0.9971 | va_acc=0.8821 | f1_macro=0.8312\n",
      "[resnet50] e014/300 | tr_loss=0.0369 | va_loss=0.1615 | tr_acc=0.9905 | va_acc=0.9392 | f1_macro=0.8978\n",
      "[resnet50] e015/300 | tr_loss=0.0335 | va_loss=0.1995 | tr_acc=0.9933 | va_acc=0.9430 | f1_macro=0.8868\n",
      "ðŸš€ Nuevo mejor F1-Macro: 0.9421 en la Ã©poca 16. Guardando checkpoint...\n",
      "[resnet50] e016/300 | tr_loss=0.0202 | va_loss=0.0946 | tr_acc=0.9971 | va_acc=0.9658 | f1_macro=0.9421\n",
      "[resnet50] e017/300 | tr_loss=0.0156 | va_loss=0.1044 | tr_acc=0.9981 | va_acc=0.9582 | f1_macro=0.9180\n",
      "[resnet50] e018/300 | tr_loss=0.0216 | va_loss=0.2844 | tr_acc=0.9952 | va_acc=0.9125 | f1_macro=0.8624\n",
      "[resnet50] e019/300 | tr_loss=0.0190 | va_loss=0.3223 | tr_acc=0.9943 | va_acc=0.8935 | f1_macro=0.8407\n",
      "[resnet50] e020/300 | tr_loss=0.0116 | va_loss=0.1344 | tr_acc=0.9990 | va_acc=0.9468 | f1_macro=0.8844\n",
      "[resnet50] e021/300 | tr_loss=0.0140 | va_loss=0.0879 | tr_acc=0.9990 | va_acc=0.9582 | f1_macro=0.9225\n",
      "[resnet50] e022/300 | tr_loss=0.0055 | va_loss=0.0948 | tr_acc=1.0000 | va_acc=0.9696 | f1_macro=0.9355\n",
      "[resnet50] e023/300 | tr_loss=0.0071 | va_loss=0.0942 | tr_acc=0.9990 | va_acc=0.9696 | f1_macro=0.9355\n",
      "[resnet50] e024/300 | tr_loss=0.0085 | va_loss=0.1411 | tr_acc=0.9990 | va_acc=0.9506 | f1_macro=0.8928\n",
      "ðŸš€ Nuevo mejor F1-Macro: 0.9527 en la Ã©poca 25. Guardando checkpoint...\n",
      "[resnet50] e025/300 | tr_loss=0.0080 | va_loss=0.0897 | tr_acc=0.9981 | va_acc=0.9772 | f1_macro=0.9527\n",
      "[resnet50] e026/300 | tr_loss=0.0066 | va_loss=0.1009 | tr_acc=0.9990 | va_acc=0.9696 | f1_macro=0.9355\n",
      "[resnet50] e027/300 | tr_loss=0.0052 | va_loss=0.0842 | tr_acc=1.0000 | va_acc=0.9696 | f1_macro=0.9355\n",
      "[resnet50] e028/300 | tr_loss=0.0095 | va_loss=0.1011 | tr_acc=0.9990 | va_acc=0.9658 | f1_macro=0.9288\n",
      "[resnet50] e029/300 | tr_loss=0.0062 | va_loss=0.0976 | tr_acc=1.0000 | va_acc=0.9696 | f1_macro=0.9378\n",
      "[resnet50] e030/300 | tr_loss=0.0045 | va_loss=0.0931 | tr_acc=1.0000 | va_acc=0.9696 | f1_macro=0.9348\n",
      "[resnet50] e031/300 | tr_loss=0.0069 | va_loss=0.0984 | tr_acc=0.9990 | va_acc=0.9620 | f1_macro=0.9180\n",
      "[resnet50] e032/300 | tr_loss=0.0067 | va_loss=0.1217 | tr_acc=1.0000 | va_acc=0.9658 | f1_macro=0.9282\n",
      "[resnet50] e033/300 | tr_loss=0.0041 | va_loss=0.1033 | tr_acc=0.9990 | va_acc=0.9658 | f1_macro=0.9341\n",
      "[resnet50] e034/300 | tr_loss=0.0037 | va_loss=0.0981 | tr_acc=1.0000 | va_acc=0.9620 | f1_macro=0.9221\n",
      "[resnet50] e035/300 | tr_loss=0.0023 | va_loss=0.1036 | tr_acc=1.0000 | va_acc=0.9658 | f1_macro=0.9288\n",
      "[resnet50] e036/300 | tr_loss=0.0029 | va_loss=0.1106 | tr_acc=1.0000 | va_acc=0.9658 | f1_macro=0.9305\n",
      "[resnet50] e037/300 | tr_loss=0.0027 | va_loss=0.1053 | tr_acc=1.0000 | va_acc=0.9620 | f1_macro=0.9270\n",
      "[resnet50] e038/300 | tr_loss=0.0028 | va_loss=0.1105 | tr_acc=1.0000 | va_acc=0.9620 | f1_macro=0.9217\n",
      "[resnet50] e039/300 | tr_loss=0.0028 | va_loss=0.1079 | tr_acc=1.0000 | va_acc=0.9658 | f1_macro=0.9341\n",
      "[resnet50] e040/300 | tr_loss=0.0017 | va_loss=0.1124 | tr_acc=1.0000 | va_acc=0.9620 | f1_macro=0.9217\n",
      "Early stopping en epoch 40 (mejor f1_macro=0.9527).\n",
      "ðŸ“„ Historial y resumen guardados.\n",
      "ðŸ“Š GrÃ¡ficas de entrenamiento guardadas.\n",
      "ðŸ“‹ Tabla LaTeX generada.\n",
      "âœ… MÃ©tricas guardadas en: D:\\PYTHON\\30_CLASIFICADOR_DE_INTERFERENCIAS\\RESULTADOS\\metrics_por_carrier.xlsx\n",
      "âœ… MÃ©tricas finales registradas en Excel.\n",
      "Modelo final cargado desde el mejor checkpoint (F1-Macro: 0.9527).\n",
      "[DONE] resnet50 | dur=5.2 min | best_f1_macro=0.9527\n",
      "\n",
      "--- Generando Artefactos de ComparaciÃ³n de Teachers ---\n",
      "[Advertencia] No se encontrÃ³ el resumen para el teacher: cnn_paper\n",
      "[Advertencia] No se encontrÃ³ el resumen para el teacher: cnn_paper_L2\n",
      "[Advertencia] No se encontrÃ³ el resumen para el teacher: resnet50\n",
      "No se encontraron resÃºmenes para generar la comparaciÃ³n.\n",
      "\n",
      "--- RESUMEN FINAL DE RENDIMIENTO DE TEACHERS ---\n",
      "  - Modelo: cnn_paper       | Mejor F1-Macro: 0.9208 (Ã‰poca 14)\n",
      "  - Modelo: cnn_paper_L2    | Mejor F1-Macro: 0.9199 (Ã‰poca 9)\n",
      "  - Modelo: resnet50        | Mejor F1-Macro: 0.9527 (Ã‰poca 25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jorge Zegarra\\AppData\\Local\\Temp\\ipykernel_16376\\1181073326.py:332: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELDA 4: Bucle Principal de Entrenamiento de Teachers\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Carga y divisiÃ³n de datos iniciales ---\n",
    "class_names = sorted([p.name for p in config.TRAIN_VAL_DIR.iterdir() if p.is_dir()])\n",
    "cls2idx = {name: i for i, name in enumerate(class_names)}\n",
    "num_classes = len(class_names)\n",
    "\n",
    "all_files, all_labels = [], []\n",
    "for class_name in class_names:\n",
    "    class_path = config.TRAIN_VAL_DIR / class_name\n",
    "    files = list(class_path.glob(\"*.png\"))\n",
    "    all_files.extend(files)\n",
    "    all_labels.extend([cls2idx[class_name]] * len(files))\n",
    "\n",
    "train_files, val_files, train_labels, val_labels = train_test_split(\n",
    "    all_files, all_labels, test_size=0.2, random_state=config.SEED, stratify=all_labels\n",
    ")\n",
    "\n",
    "train_ds = LabeledSpectro(train_files, train_labels, transform=weak_aug)\n",
    "val_ds = LabeledSpectro(val_files, val_labels, transform=None)\n",
    "print(f\"Datos listos: {len(train_ds)} muestras de entrenamiento, {len(val_ds)} de validaciÃ³n.\")\n",
    "\n",
    "# --- Bucle para entrenar cada Teacher ---\n",
    "all_teacher_results = {}\n",
    "teacher_model_keys = list(config.TRAIN_PARAMS.keys())\n",
    "\n",
    "for model_key in teacher_model_keys:\n",
    "    print(f\"\\n{'='*20} ENTRENANDO TEACHER: {model_key.upper()} {'='*20}\")\n",
    "    params = config.TRAIN_PARAMS[model_key]\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=params['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_ds, batch_size=params['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "    \n",
    "    results = run_training_session(params, train_loader, val_loader, num_classes, class_names, run_type='teacher')\n",
    "    \n",
    "    all_teacher_results[model_key] = results['helper'].best_epoch_metrics\n",
    "\n",
    "# --- FASE 2: Generar artefactos de comparaciÃ³n ---\n",
    "generate_teacher_comparison_artifacts(config.RESULTS_DIR, teacher_model_keys)\n",
    "\n",
    "# --- Imprimir resumen final en consola ---\n",
    "print(\"\\n--- RESUMEN FINAL DE RENDIMIENTO DE TEACHERS ---\")\n",
    "for model_key, metrics in all_teacher_results.items():\n",
    "    if metrics:\n",
    "        print(f\"  - Modelo: {model_key:<15} | Mejor F1-Macro: {metrics['f1m']:.4f} (Ã‰poca {metrics['epoch']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "from_bard": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
