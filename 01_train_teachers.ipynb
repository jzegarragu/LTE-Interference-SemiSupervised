{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: Teacher Models Training\n",
    "\n",
    "This notebook executes the training pipeline for the initial \"Teacher\" models (`cnn_paper`, `cnn_paper_L2`, `resnet50`, `resnet18`).\n",
    "\n",
    "**Objective:** To generate competent base classifiers using only the labeled subset (`train_val`) created by the data preparation script. These models will later serve as \"Teachers\" to generate pseudo-labels for the unlabeled data in the semi-supervised stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 2: Imports & Setup\n",
    "# ==============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix\n",
    "\n",
    "# --- Local Modules (Relative Paths handled in config.py) ---\n",
    "import config\n",
    "import models\n",
    "import utils\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(config.SEED)\n",
    "np.random.seed(config.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 3: Training Utilities and Helper Classes\n",
    "# ==============================================================================\n",
    "\n",
    "class ThesisHelper:\n",
    "    \"\"\"\n",
    "    Handles experiment logging, checkpoint saving, and artifact generation \n",
    "    (plots, tables) for thesis documentation.\n",
    "    \"\"\"\n",
    "    def __init__(self, params, class_names, base_dir, run_type='teacher'):\n",
    "        self.params = params\n",
    "        self.class_names = class_names\n",
    "        \n",
    "        # Construct unique run tag based on hyperparameters\n",
    "        lr = self.params['LR']\n",
    "        wd = self.params['WEIGHT_DECAY']\n",
    "        hparams_tag = f\"_lr{lr}_wd{wd}\"\n",
    "        \n",
    "        if run_type == 'student':\n",
    "            base_name = f\"student_trained_with_win_teachers{self.params['MODEL_NAME']}\"\n",
    "        else: \n",
    "            base_name = f\"{run_type}_{self.params['MODEL_NAME']}\"\n",
    "            \n",
    "        self.run_name = base_name + hparams_tag\n",
    "        self.output_dir = Path(base_dir) / self.run_name\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.run_type = run_type\n",
    "        \n",
    "        self.history = []\n",
    "        self.best_f1_macro = -1.0\n",
    "        self.best_epoch_metrics = None\n",
    "        \n",
    "        print(f\"[INFO] ThesisHelper initialized. Artifacts dir: {self.output_dir}\")\n",
    "\n",
    "    def log_epoch(self, model, metrics):\n",
    "        self.history.append(metrics)\n",
    "        current_f1_macro = metrics['f1m']\n",
    "        \n",
    "        if current_f1_macro > self.best_f1_macro:\n",
    "            self.best_f1_macro = current_f1_macro\n",
    "            self.best_epoch_metrics = metrics\n",
    "            print(f\"   >>> [NEW BEST] F1-Macro: {self.best_f1_macro:.4f} (Epoch {metrics['epoch']}). Saving checkpoint...\")\n",
    "            self._save_checkpoint(model)\n",
    "\n",
    "    def _save_checkpoint(self, model):\n",
    "        torch.save(model.state_dict(), self.output_dir / 'best_model.pth')\n",
    "\n",
    "    def finalize(self, total_duration_seconds):\n",
    "        if not self.history:\n",
    "            print(\"[WARN] No history to finalize.\")\n",
    "            return\n",
    "\n",
    "        # 1. Save History CSV\n",
    "        history_df = pd.DataFrame(self.history)\n",
    "        history_df.to_csv(self.output_dir / 'training_history.csv', index=False)\n",
    "        \n",
    "        # 2. Save JSON Summary\n",
    "        summary = self.best_epoch_metrics.copy()\n",
    "        summary['total_duration_min'] = total_duration_seconds / 60\n",
    "        cm = summary.pop('cm', None) \n",
    "        \n",
    "        with open(self.output_dir / 'summary.json', 'w') as f:\n",
    "            json.dump(summary, f, indent=4)\n",
    "        \n",
    "        # 3. Generate Artifacts\n",
    "        self._plot_curves(history_df)\n",
    "        self._generate_latex_table(summary)\n",
    "        self._log_to_excel(summary, cm)\n",
    "        \n",
    "        print(f\"[INFO] Experiment finalized. All artifacts saved in {self.output_dir}\")\n",
    "\n",
    "    def _plot_curves(self, df):\n",
    "        best_epoch = self.best_epoch_metrics['epoch']\n",
    "        \n",
    "        plt.style.use('seaborn-v0_8-whitegrid')\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
    "        \n",
    "        # Loss Curve\n",
    "        ax1.plot(df['epoch'], df['tr_loss'], 'o-', label='Training Loss', markersize=4)\n",
    "        ax1.plot(df['epoch'], df['loss'], 'o-', label='Validation Loss', markersize=4)\n",
    "        ax1.axvline(x=best_epoch, color='r', linestyle='--', alpha=0.5, label=f'Best Epoch ({best_epoch})')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title(f'Training Curves: {self.run_name}')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "        \n",
    "        # Metrics Curve\n",
    "        ax2.plot(df['epoch'], df['tr_acc'], 'o-', label='Train Accuracy', markersize=4, alpha=0.7)\n",
    "        ax2.plot(df['epoch'], df['acc'], 'o-', label='Val Accuracy', markersize=4, alpha=0.7)\n",
    "        ax2.plot(df['epoch'], df['f1m'], 'o-', label='Val F1-Macro', linewidth=2, markersize=6, color='green')\n",
    "        ax2.axvline(x=best_epoch, color='r', linestyle='--', alpha=0.5)\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Metric Score')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, linestyle='--', alpha=0.6)\n",
    "        ax2.yaxis.set_major_formatter(mticker.PercentFormatter(xmax=1.0))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir / 'training_curves.png', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def _generate_latex_table(self, summary):\n",
    "        # Generates a ready-to-copy LaTeX table for the paper\n",
    "        latex_str = f\"\"\"\n",
    "\\\\begin{{table}}[h!]\n",
    "\\\\centering\n",
    "\\\\caption{{Training summary for model {self.run_name.replace('_', ' ')}. Metrics reported at the best epoch.}}\n",
    "\\\\label{{tab:training_summary_{self.run_name}}}\n",
    "\\\\begin{{tabular}}{{ll}}\n",
    "\\\\hline\n",
    "\\\\textbf{{Parameter}} & \\\\textbf{{Value}} \\\\\\\\\n",
    "\\\\hline\n",
    "Base Architecture & {self.params['MODEL_NAME']} \\\\\\\\\n",
    "Best Epoch & {summary['epoch']} \\\\\\\\\n",
    "Total Duration (min) & {summary['total_duration_min']:.2f} \\\\\\\\\n",
    "\\\\hline\n",
    "\\\\textbf{{Validation Metrics}} & \\\\textbf{{Value}} \\\\\\\\\n",
    "\\\\hline\n",
    "F1-Macro (Best) & {summary['f1m']:.4f} \\\\\\\\\n",
    "Accuracy & {summary['acc']:.4f} \\\\\\\\\n",
    "Loss & {summary['loss']:.4f} \\\\\\\\\n",
    "Recall (Macro) & {summary['recm']:.4f} \\\\\\\\\n",
    "\\\\hline\n",
    "\\\\end{{tabular}}\n",
    "\\\\end{{table}}\n",
    "        \"\"\"\n",
    "        with open(self.output_dir / 'summary_table.tex', 'w') as f:\n",
    "            f.write(latex_str)\n",
    "\n",
    "    def _log_to_excel(self, summary, cm):\n",
    "        note = f\"{self.run_type.capitalize()} - Best Checkpoint at Epoch {summary['epoch']}\"\n",
    "        \n",
    "        metrics_to_log = {\n",
    "            'carrier': config.CURRENT_CARRIER,\n",
    "            'model_name': self.params['MODEL_NAME'],\n",
    "            'run_tag': self.run_name,\n",
    "            'num_classes': len(self.class_names),\n",
    "            'acc': summary['acc'],\n",
    "            'loss': summary['loss'],\n",
    "            'f1m': summary['f1m'],\n",
    "            'f1w': summary['f1w'],\n",
    "            'recm': summary['recm'],\n",
    "            'cm': cm,\n",
    "            'epochs': self.params['EPOCHS'],\n",
    "            'batch_size': self.params['BATCH_SIZE'],\n",
    "            'lr': self.params['LR'],\n",
    "            'weight_decay': self.params['WEIGHT_DECAY'],\n",
    "            'notes': note\n",
    "        }\n",
    "        utils.log_metrics_excel(config.METRICS_FILE, config.ARTIFACTS_DIR, self.class_names, metrics_to_log)\n",
    "\n",
    "# --- Data Augmentation ---\n",
    "class RandomTimeShift(torch.nn.Module):\n",
    "    \"\"\"Applies a random cyclic shift along the time axis.\"\"\"\n",
    "    def __init__(self, max_frac=0.1):\n",
    "        super().__init__()\n",
    "        self.max_frac = max_frac\n",
    "    def forward(self, x):\n",
    "        _, H, W = x.shape\n",
    "        s = int(random.uniform(-self.max_frac, self.max_frac) * W)\n",
    "        return torch.roll(x, shifts=s, dims=-1)\n",
    "\n",
    "class RandomGain(torch.nn.Module):\n",
    "    \"\"\"Applies a random gain (multiplication) to pixel intensity.\"\"\"\n",
    "    def __init__(self, a=0.95, b=1.05):\n",
    "        super().__init__()\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "    def forward(self, x):\n",
    "        g = random.uniform(self.a, self.b)\n",
    "        return (x * g).clamp(0, 1)\n",
    "\n",
    "weak_aug = T.Compose([\n",
    "    RandomTimeShift(0.08),\n",
    "    RandomGain(0.95, 1.05),\n",
    "])\n",
    "\n",
    "class LabeledSpectro(Dataset):\n",
    "    def __init__(self, files, labels, transform=None):\n",
    "        self.files = files\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    def __getitem__(self, i):\n",
    "        path = self.files[i]\n",
    "        try:\n",
    "            x = utils.load_png_gray(path)\n",
    "            if self.transform:\n",
    "                x = self.transform(x)\n",
    "            y = self.labels[i]\n",
    "            return x, y\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Skipping file due to error: {path} -> {e}\")\n",
    "            # Return dummy tensor to maintain batch integrity\n",
    "            return torch.zeros(1, config.IMG_SIZE[0], config.IMG_SIZE[1]), self.labels[i]\n",
    "\n",
    "def maybe_resize_for_resnet(x, should_resize):\n",
    "    if should_resize:\n",
    "        return torch.nn.functional.interpolate(x, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "    return x\n",
    "\n",
    "# --- Early Stopping & Evaluation ---\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience, min_delta, mode='max', restore_best=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.restore_best = restore_best\n",
    "        self.best = -float('inf') if mode == 'max' else float('inf')\n",
    "        self.wait = 0\n",
    "        self.best_state = None\n",
    "\n",
    "    def step(self, metric, model):\n",
    "        is_better = (metric > self.best + self.min_delta) if self.mode == 'max' else (metric < self.best - self.min_delta)\n",
    "        if is_better:\n",
    "            self.best = metric\n",
    "            self.wait = 0\n",
    "            if self.restore_best:\n",
    "                self.best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            return False \n",
    "        self.wait += 1\n",
    "        return self.wait >= self.patience\n",
    "\n",
    "    def restore(self, model):\n",
    "        if self.restore_best and self.best_state is not None:\n",
    "            model.load_state_dict(self.best_state)\n",
    "\n",
    "def evaluate(model, loader, criterion, params):\n",
    "    device = torch.device(config.DEVICE)\n",
    "    model.eval()\n",
    "    va_loss, preds, gts = 0.0, [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = maybe_resize_for_resnet(xb, params.get('RESIZE_224', False))\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            va_loss += loss.item() * xb.size(0)\n",
    "            preds.append(logits.softmax(1).argmax(1).cpu())\n",
    "            gts.append(yb.cpu())\n",
    "            \n",
    "    va_loss /= len(loader.dataset)\n",
    "    y_pred = torch.cat(preds).numpy()\n",
    "    y_true = torch.cat(gts).numpy()\n",
    "    \n",
    "    return {\n",
    "        'loss': va_loss,\n",
    "        'acc': accuracy_score(y_true, y_pred),\n",
    "        'f1m': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'f1w': f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'recm': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'cm': confusion_matrix(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "def run_training_session(params, train_loader, val_loader, num_classes, class_names, run_type):\n",
    "    device = torch.device(config.DEVICE)\n",
    "    helper = ThesisHelper(params, class_names, base_dir=config.ARTIFACTS_DIR, run_type=run_type)\n",
    "    \n",
    "    model = models.make_model(\n",
    "        params['MODEL_NAME'], \n",
    "        num_classes, \n",
    "        params.get('USE_PRETRAIN', True)\n",
    "    ).to(device)\n",
    "    \n",
    "    opt = torch.optim.SGD(model.parameters(), lr=params['LR'], momentum=params['MOMENTUM'], weight_decay=params['WEIGHT_DECAY'])\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=params['EPOCHS'], eta_min=params['LR'] * config.ETA_MIN_FACTOR)\n",
    "    es = EarlyStopping(patience=params['PATIENCE'], min_delta=config.EARLY_STOPPING_CONFIG['min_delta'], restore_best=False)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    for ep in range(1, params['EPOCHS'] + 1):\n",
    "        model.train()\n",
    "        tr_loss, n = 0.0, 0\n",
    "        tr_preds, tr_gts = [], []\n",
    "        \n",
    "        for xb, yb in train_loader:\n",
    "            xb = maybe_resize_for_resnet(xb, params.get('RESIZE_224', False))\n",
    "            xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "            \n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=config.CLIP_MAX_NORM)\n",
    "            opt.step()\n",
    "            \n",
    "            tr_loss += loss.item() * xb.size(0)\n",
    "            n += xb.size(0)\n",
    "            tr_preds.append(logits.softmax(1).argmax(1).cpu())\n",
    "            tr_gts.append(yb.cpu())\n",
    "            \n",
    "        tr_loss /= n\n",
    "        sched.step()\n",
    "        \n",
    "        # Calculate Train Accuracy\n",
    "        tr_acc = accuracy_score(torch.cat(tr_gts).numpy(), torch.cat(tr_preds).numpy())\n",
    "        \n",
    "        # Validation\n",
    "        val_metrics = evaluate(model, val_loader, crit, params)\n",
    "        monitor_metric_val = val_metrics['f1m'] # Defaulting to f1m as per config\n",
    "        \n",
    "        log_entry = {\n",
    "            'epoch': ep, 'tr_loss': tr_loss, 'tr_acc': tr_acc, \n",
    "            **val_metrics, 'lr': sched.get_last_lr()[0]\n",
    "        }\n",
    "        helper.log_epoch(model, log_entry)\n",
    "        \n",
    "        print(f\"[{params['MODEL_NAME']}] Ep {ep:03d}/{params['EPOCHS']} | Train Loss: {tr_loss:.4f} | Val Loss: {val_metrics['loss']:.4f} | Val F1-Macro: {monitor_metric_val:.4f}\")\n",
    "        \n",
    "        if es.step(monitor_metric_val, model):\n",
    "            print(f\"[INFO] Early stopping triggered at epoch {ep}.\")\n",
    "            break\n",
    "            \n",
    "    dur = time.time() - t0\n",
    "    helper.finalize(dur)\n",
    "    \n",
    "    # Reload best model\n",
    "    best_model_path = helper.output_dir / 'best_model.pth'\n",
    "    if best_model_path.exists():\n",
    "        model.load_state_dict(torch.load(best_model_path))\n",
    "        print(f\"[DONE] Best model loaded (F1-Macro: {helper.best_f1_macro:.4f}).\")\n",
    "        \n",
    "    return {'model': model, 'helper': helper}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 4: Comparative Analysis Artifacts\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_teacher_comparison_artifacts(results_dir, teacher_keys):\n",
    "    \"\"\"\n",
    "    Aggregates summaries from all teachers and generates comparison charts and LaTeX tables.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Generating Teacher Comparison Artifacts ---\")\n",
    "    summaries = []\n",
    "    \n",
    "    for key in teacher_keys:\n",
    "        # Note: We need to reconstruct the folder name. Assuming standard params for identification.\n",
    "        # Ideally, we should track the exact output folder names during training.\n",
    "        # For this script, we scan the directory for folders matching the model name.\n",
    "        found = False\n",
    "        for path in Path(results_dir).iterdir():\n",
    "            if path.is_dir() and f\"teacher_{key}\" in path.name:\n",
    "                summary_path = path / \"summary.json\"\n",
    "                if summary_path.exists():\n",
    "                    with open(summary_path, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "                        data['model_key'] = key\n",
    "                        summaries.append(data)\n",
    "                        found = True\n",
    "                        break\n",
    "        if not found:\n",
    "            print(f\"[WARN] Summary not found for teacher: {key}\")\n",
    "\n",
    "    if not summaries:\n",
    "        print(\"[ERROR] No summaries found. Cannot generate comparison.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(summaries).sort_values('f1m', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # 1. Bar Chart\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Highlight the best model\n",
    "    colors = ['#ff7f0e' if i == 0 else '#1f77b4' for i in df.index]\n",
    "    bars = ax.bar(df['model_key'], df['f1m'], color=colors)\n",
    "    \n",
    "    ax.bar_label(bars, fmt='%.4f', padding=3)\n",
    "    ax.set_ylabel('F1-Score Macro (Validation)')\n",
    "    ax.set_xlabel('Teacher Model Architecture')\n",
    "    ax.set_title('Performance Comparison of Teacher Models')\n",
    "    ax.set_ylim(0, max(df['f1m']) * 1.1)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plot_path = Path(results_dir) / \"comparison_teachers_performance.png\"\n",
    "    plt.savefig(plot_path, dpi=300)\n",
    "    print(f\"Comparison chart saved to: {plot_path}\")\n",
    "    plt.close()\n",
    "\n",
    "    # 2. LaTeX Table\n",
    "    df_latex = df[['model_key', 'f1m', 'acc', 'loss']].copy()\n",
    "    df_latex.columns = ['Model', 'F1-Macro', 'Accuracy', 'Loss']\n",
    "    \n",
    "    # Bold the best model name\n",
    "    df_latex['Model'] = df_latex.apply(lambda row: f\"\\\\textbf{{{row.Model}}}\" if row.name == 0 else row.Model, axis=1)\n",
    "\n",
    "    latex_table = df_latex.to_latex(\n",
    "        index=False, \n",
    "        float_format=\"%.4f\", \n",
    "        caption=\"Validation performance comparison of Teacher models. The best model (by F1-Macro) is highlighted in bold.\", \n",
    "        label=\"tab:teacher_comparison\", \n",
    "        position=\"h!\"\n",
    "    )\n",
    "    \n",
    "    table_path = Path(results_dir) / \"comparison_teachers_table.tex\"\n",
    "    with open(table_path, 'w') as f:\n",
    "        f.write(latex_table)\n",
    "    print(f\"LaTeX table saved to: {table_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 5: Main Execution Pipeline\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. Data Loading & Splitting\n",
    "# ---------------------------\n",
    "if not config.TRAIN_VAL_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Data directory not found at {config.TRAIN_VAL_DIR}. Did you run 00_prepare_data.py?\")\n",
    "\n",
    "class_names = sorted([p.name for p in config.TRAIN_VAL_DIR.iterdir() if p.is_dir()])\n",
    "cls2idx = {name: i for i, name in enumerate(class_names)}\n",
    "num_classes = len(class_names)\n",
    "\n",
    "all_files, all_labels = [], []\n",
    "for class_name in class_names:\n",
    "    class_path = config.TRAIN_VAL_DIR / class_name\n",
    "    files = list(class_path.glob(\"*.png\"))\n",
    "    all_files.extend(files)\n",
    "    all_labels.extend([cls2idx[class_name]] * len(files))\n",
    "\n",
    "# Stratified Split (80% Train, 20% Val)\n",
    "train_files, val_files, train_labels, val_labels = train_test_split(\n",
    "    all_files, all_labels, test_size=0.2, random_state=config.SEED, stratify=all_labels\n",
    ")\n",
    "\n",
    "train_ds = LabeledSpectro(train_files, train_labels, transform=weak_aug)\n",
    "val_ds = LabeledSpectro(val_files, val_labels, transform=None)\n",
    "\n",
    "print(f\"[INFO] Dataset Loaded. Train: {len(train_ds)} | Val: {len(val_ds)} | Classes: {num_classes}\")\n",
    "\n",
    "# 2. Training Loop\n",
    "# ----------------\n",
    "all_teacher_results = {}\n",
    "teacher_model_keys = list(config.TRAIN_PARAMS.keys())\n",
    "\n",
    "for model_key in teacher_model_keys:\n",
    "    print(f\"\\n{'-'*20} TRAINING TEACHER: {model_key.upper()} {'-'*20}\")\n",
    "    \n",
    "    params = config.TRAIN_PARAMS[model_key]\n",
    "    \n",
    "    # Update paths in params if necessary or just rely on config\n",
    "    train_loader = DataLoader(train_ds, batch_size=params['BATCH_SIZE'], shuffle=True, num_workers=config.NUM_WORKERS)\n",
    "    val_loader = DataLoader(val_ds, batch_size=params['BATCH_SIZE'], shuffle=False, num_workers=config.NUM_WORKERS)\n",
    "    \n",
    "    results = run_training_session(\n",
    "        params, \n",
    "        train_loader, \n",
    "        val_loader, \n",
    "        num_classes, \n",
    "        class_names, \n",
    "        run_type='teacher'\n",
    "    )\n",
    "    \n",
    "    all_teacher_results[model_key] = results['helper'].best_epoch_metrics\n",
    "\n",
    "# 3. Final Comparison\n",
    "# -------------------\n",
    "generate_teacher_comparison_artifacts(config.ARTIFACTS_DIR, teacher_model_keys)\n",
    "\n",
    "print(\"\\n--- FINAL TEACHER PERFORMANCE SUMMARY ---\")\n",
    "for model_key, metrics in all_teacher_results.items():\n",
    "    if metrics:\n",
    "        print(f\"  > {model_key:<15} | Best F1-Macro: {metrics['f1m']:.4f} (Epoch {metrics['epoch']})\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "from_bard": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
