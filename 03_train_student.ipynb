{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee8bb065-dfa8-4092-a92a-8ac53b2e4f02",
   "metadata": {},
   "source": [
    "# Stage 3: Student Model Training\n",
    "\n",
    "This notebook trains the final \"Student\" model using a **data augmentation strategy** based on Semi-Supervised Learning.\n",
    "\n",
    "**Methodology:**\n",
    "* **Dataset:** A combination of the original labeled data (`train_val`) and high-quality pseudo-labels generated by the \"Teacher\" models.\n",
    "* **Objective:** To demonstrate that the Student model, by learning from a larger volume of data (including unverified but high-confidence samples), achieves better generalization performance than the original Teachers.\n",
    "* **Experiment:** We perform a \"Pure Lineage\" training, where a Student architecture (e.g., ResNet18) learns specifically from the pseudo-labels generated by its homologous Teacher (ResNet18)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486b9e9a-6075-4b7e-8a78-aba07c8cb9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 2: Imports & Setup\n",
    "# ==============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix\n",
    "\n",
    "# --- Local Modules ---\n",
    "import config\n",
    "import models\n",
    "import utils\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(config.SEED)\n",
    "np.random.seed(config.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f828d0-ee7b-4774-9e4f-0bc7644f80f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 3: ThesisHelper Class\n",
    "# ==============================================================================\n",
    "class ThesisHelper:\n",
    "    \"\"\"\n",
    "    Manages experiment logging, checkpointing, and artifact generation \n",
    "    (plots, LaTeX tables) for thesis documentation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, class_names, base_dir, run_type='teacher'):\n",
    "        self.params = params\n",
    "        self.class_names = class_names\n",
    "        \n",
    "        # Construct run name based on execution type\n",
    "        if run_type == 'student':\n",
    "            self.run_name = f\"student_trained_with_win_teachers{params['MODEL_NAME']}\"\n",
    "        else: \n",
    "            self.run_name = f\"{run_type}_{params['MODEL_NAME']}\"\n",
    "            \n",
    "        self.output_dir = Path(base_dir) / self.run_name\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.run_type = run_type\n",
    "        \n",
    "        self.history = []\n",
    "        self.best_f1_macro = -1.0\n",
    "        self.best_epoch_metrics = None\n",
    "        \n",
    "        print(f\"[INFO] ThesisHelper initialized for '{self.run_name}'. Artifacts dir: {self.output_dir}\")\n",
    "\n",
    "    def log_epoch(self, model, metrics):\n",
    "        \"\"\"Logs epoch metrics and saves the best checkpoint based on F1-Macro.\"\"\"\n",
    "        self.history.append(metrics)\n",
    "        current_f1_macro = metrics['f1m']\n",
    "        \n",
    "        if current_f1_macro > self.best_f1_macro:\n",
    "            self.best_f1_macro = current_f1_macro\n",
    "            self.best_epoch_metrics = metrics\n",
    "            print(f\"[INFO] New best F1-Macro: {self.best_f1_macro:.4f} (Epoch {metrics['epoch']}). Saving checkpoint...\")\n",
    "            self._save_checkpoint(model)\n",
    "\n",
    "    def _save_checkpoint(self, model):\n",
    "        \"\"\"Saves the model state dictionary.\"\"\"\n",
    "        torch.save(model.state_dict(), self.output_dir / 'best_model.pth')\n",
    "\n",
    "    def finalize(self, total_duration_seconds):\n",
    "        \"\"\"Generates all final artifacts after training completes.\"\"\"\n",
    "        if not self.history:\n",
    "            print(\"[WARN] No history to finalize. Skipping artifact generation.\")\n",
    "            return\n",
    "\n",
    "        # 1. Save History and Summary\n",
    "        history_df = pd.DataFrame(self.history)\n",
    "        history_df.to_csv(self.output_dir / 'training_history.csv', index=False)\n",
    "        \n",
    "        summary = self.best_epoch_metrics.copy()\n",
    "        summary['total_duration_min'] = total_duration_seconds / 60\n",
    "        cm = summary.pop('cm', None) \n",
    "        \n",
    "        with open(self.output_dir / 'summary.json', 'w') as f:\n",
    "            json.dump(summary, f, indent=4)\n",
    "            \n",
    "        print(\"[INFO] History and summary saved.\")\n",
    "\n",
    "        # 2. Generate Plots\n",
    "        self._plot_curves(history_df)\n",
    "        print(\"[INFO] Training curves plot saved.\")\n",
    "        \n",
    "        # 3. Generate LaTeX Table\n",
    "        self._generate_latex_table(summary, cm)\n",
    "        print(\"[INFO] LaTeX table generated.\")\n",
    "        \n",
    "        # 4. Log to Main Excel\n",
    "        self._log_to_excel(summary, cm)\n",
    "        print(\"[INFO] Final metrics appended to Master Excel file.\")\n",
    "\n",
    "    def _plot_curves(self, df):\n",
    "        \"\"\"Generates and saves training vs validation curves.\"\"\"\n",
    "        best_epoch = self.best_epoch_metrics['epoch']\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
    "        \n",
    "        # Subplot 1: Loss\n",
    "        ax1.plot(df['epoch'], df['tr_loss'], 'o-', label='Training Loss')\n",
    "        ax1.plot(df['epoch'], df['loss'], 'o-', label='Validation Loss')\n",
    "        ax1.axvline(x=best_epoch, color='r', linestyle='--', label=f'Best Epoch ({best_epoch})')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "        \n",
    "        # Subplot 2: Metrics\n",
    "        ax2.plot(df['epoch'], df['tr_acc'], 'o-', label='Training Accuracy')\n",
    "        ax2.plot(df['epoch'], df['acc'], 'o-', label='Validation Accuracy')\n",
    "        ax2.plot(df['epoch'], df['f1m'], 'o-', label='Validation F1-Macro', linewidth=2, markersize=8)\n",
    "        ax2.axvline(x=best_epoch, color='r', linestyle='--')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Metric')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, linestyle='--', alpha=0.6)\n",
    "        ax2.yaxis.set_major_formatter(mticker.PercentFormatter(xmax=1.0))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir / 'training_curves.png', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def _generate_latex_table(self, summary, cm):\n",
    "        \"\"\"Generates a LaTeX table summarizing the run.\"\"\"\n",
    "        latex_str = f\"\"\"\n",
    "\\\\begin{{table}}[h!]\n",
    "\\\\centering\n",
    "\\\\caption{{Training summary for model {self.run_name.replace('_', ' ')}. Metrics reported at the best epoch.}}\n",
    "\\\\label{{tab:training_summary_{self.run_name}}}\n",
    "\\\\begin{{tabular}}{{ll}}\n",
    "\\\\hline\n",
    "\\\\textbf{{Parameter}} & \\\\textbf{{Value}} \\\\\\\\\n",
    "\\\\hline\n",
    "Base Architecture & {self.params['MODEL_NAME']} \\\\\\\\\n",
    "Best Epoch & {summary['epoch']} \\\\\\\\\n",
    "Total Duration (min) & {summary['total_duration_min']:.2f} \\\\\\\\\n",
    "\\\\hline\n",
    "\\\\textbf{{Validation Metrics}} & \\\\textbf{{Value}} \\\\\\\\\n",
    "\\\\hline\n",
    "F1-Macro (Best) & {summary['f1m']:.4f} \\\\\\\\\n",
    "Accuracy & {summary['acc']:.4f} \\\\\\\\\n",
    "Loss & {summary['loss']:.4f} \\\\\\\\\n",
    "Recall (Macro) & {summary['recm']:.4f} \\\\\\\\\n",
    "\\\\hline\n",
    "\\\\end{{tabular}}\n",
    "\\\\end{{table}}\n",
    "        \"\"\"\n",
    "        with open(self.output_dir / 'summary_table.tex', 'w') as f:\n",
    "            f.write(latex_str)\n",
    "\n",
    "    def _log_to_excel(self, summary, cm):\n",
    "        \"\"\"Appends metrics to the master Excel file.\"\"\"\n",
    "        metrics_to_log = {\n",
    "            'carrier': config.CURRENT_CARRIER,\n",
    "            'model_name': self.params['MODEL_NAME'],\n",
    "            'run_tag': self.run_name,\n",
    "            'num_classes': len(self.class_names),\n",
    "            'acc': summary['acc'],\n",
    "            'loss': summary['loss'],\n",
    "            'f1m': summary['f1m'],\n",
    "            'f1w': summary['f1w'],\n",
    "            'recm': summary['recm'],\n",
    "            'cm': cm,\n",
    "            'epochs': self.params['EPOCHS'],\n",
    "            'batch_size': self.params['BATCH_SIZE'],\n",
    "            'lr': self.params['LR'],\n",
    "            'weight_decay': self.params['WEIGHT_DECAY'],\n",
    "            'notes': f\"Student - Best checkpoint at epoch {summary['epoch']}\"\n",
    "        }\n",
    "        utils.log_metrics_excel(config.METRICS_FILE, config.ARTIFACTS_DIR, self.class_names, metrics_to_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9795eef8-c18f-4cfc-b850-3bf714a2f2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 4: Training Components & Student Loop\n",
    "# ==============================================================================\n",
    "\n",
    "class RandomTimeShift(torch.nn.Module):\n",
    "    def __init__(self, max_frac=0.1):\n",
    "        super().__init__()\n",
    "        self.max_frac = max_frac\n",
    "    def forward(self, x):\n",
    "        _, H, W = x.shape\n",
    "        s = int(random.uniform(-self.max_frac, self.max_frac) * W)\n",
    "        return torch.roll(x, shifts=s, dims=-1)\n",
    "\n",
    "class RandomGain(torch.nn.Module):\n",
    "    def __init__(self, a=0.95, b=1.05):\n",
    "        super().__init__()\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "    def forward(self, x):\n",
    "        g = random.uniform(self.a, self.b)\n",
    "        return (x * g).clamp(0, 1)\n",
    "\n",
    "weak_aug = T.Compose([\n",
    "    RandomTimeShift(0.08),\n",
    "    RandomGain(0.95, 1.05),\n",
    "])\n",
    "\n",
    "class LabeledSpectro(Dataset):\n",
    "    def __init__(self, files, labels, transform=None):\n",
    "        self.files = files\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    def __getitem__(self, i):\n",
    "        path = self.files[i]\n",
    "        try:\n",
    "            x = utils.load_png_gray(path)\n",
    "            if self.transform:\n",
    "                x = self.transform(x)\n",
    "            y = self.labels[i]\n",
    "            return x, y\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Skipping file due to error: {e}\")\n",
    "            # Return dummy tensor to preserve batch integrity\n",
    "            return torch.zeros(1, config.IMG_SIZE[0], config.IMG_SIZE[1], dtype=torch.float32), self.labels[i]\n",
    "\n",
    "def maybe_resize_for_resnet(x, should_resize):\n",
    "    if should_resize:\n",
    "        return torch.nn.functional.interpolate(x, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "    return x\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience, min_delta, mode='max', restore_best=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.restore_best = restore_best\n",
    "        self.best = -float('inf') if mode == 'max' else float('inf')\n",
    "        self.wait = 0\n",
    "        self.best_state = None\n",
    "\n",
    "    def step(self, metric, model):\n",
    "        is_better = (metric > self.best + self.min_delta) if self.mode == 'max' else (metric < self.best - self.min_delta)\n",
    "        if is_better:\n",
    "            self.best = metric\n",
    "            self.wait = 0\n",
    "            if self.restore_best:\n",
    "                self.best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            return False\n",
    "        self.wait += 1\n",
    "        return self.wait >= self.patience\n",
    "\n",
    "    def restore(self, model):\n",
    "        if self.restore_best and self.best_state is not None:\n",
    "            model.load_state_dict(self.best_state)\n",
    "\n",
    "def evaluate(model, loader, criterion, params):\n",
    "    device = torch.device(config.DEVICE)\n",
    "    model.eval()\n",
    "    va_loss, preds, gts = 0.0, [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = maybe_resize_for_resnet(xb, params.get('RESIZE_224', False))\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            va_loss += loss.item() * xb.size(0)\n",
    "            preds.append(logits.softmax(1).argmax(1).cpu())\n",
    "            gts.append(yb.cpu())\n",
    "    va_loss /= len(loader.dataset)\n",
    "    y_pred = torch.cat(preds).numpy()\n",
    "    y_true = torch.cat(gts).numpy()\n",
    "    \n",
    "    return {\n",
    "        'loss': va_loss,\n",
    "        'acc': accuracy_score(y_true, y_pred),\n",
    "        'f1m': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'f1w': f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'recm': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'cm': confusion_matrix(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "def train_student(params, train_loader, val_loader, num_classes, class_names):\n",
    "    device = torch.device(config.DEVICE)\n",
    "    torch.manual_seed(config.SEED)\n",
    "    np.random.seed(config.SEED)\n",
    "\n",
    "    helper = ThesisHelper(params, class_names, base_dir=config.ARTIFACTS_DIR, run_type='student')\n",
    "\n",
    "    model = models.make_model(\n",
    "        params['MODEL_NAME'], \n",
    "        num_classes, \n",
    "        params.get('USE_PRETRAIN', True)\n",
    "    ).to(device)\n",
    "    \n",
    "    opt = torch.optim.SGD(model.parameters(), lr=params['LR'], momentum=params['MOMENTUM'], weight_decay=params['WEIGHT_DECAY'])\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=params['EPOCHS'], eta_min=params['LR'] * config.ETA_MIN_FACTOR)\n",
    "    \n",
    "    es = EarlyStopping(patience=params['PATIENCE'], min_delta=config.EARLY_STOPPING_CONFIG['min_delta'], restore_best=False)\n",
    "    t0 = time.time()\n",
    "\n",
    "    for ep in range(1, params['EPOCHS'] + 1):\n",
    "        model.train()\n",
    "        tr_loss, n = 0.0, 0\n",
    "        tr_preds, tr_gts = [], []\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb = maybe_resize_for_resnet(xb, params.get('RESIZE_224', False))\n",
    "            xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "            \n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=config.CLIP_MAX_NORM)\n",
    "            opt.step()\n",
    "            \n",
    "            tr_loss += loss.item() * xb.size(0)\n",
    "            n += xb.size(0)\n",
    "            \n",
    "            tr_preds.append(logits.softmax(1).argmax(1).cpu())\n",
    "            tr_gts.append(yb.cpu())\n",
    "        \n",
    "        tr_loss /= n\n",
    "        sched.step()\n",
    "\n",
    "        # Calculate Train Accuracy\n",
    "        y_pred_tr = torch.cat(tr_preds).numpy()\n",
    "        y_true_tr = torch.cat(tr_gts).numpy()\n",
    "        tr_acc = accuracy_score(y_true_tr, y_pred_tr)\n",
    "\n",
    "        # Validation\n",
    "        val_metrics = evaluate(model, val_loader, crit, params)\n",
    "        monitor_metric_val = val_metrics['f1m'] # Defaulting to f1m\n",
    "        \n",
    "        log_entry = {\n",
    "            'epoch': ep, 'tr_loss': tr_loss, 'tr_acc': tr_acc, \n",
    "            **val_metrics, 'lr': sched.get_last_lr()[0]\n",
    "        }\n",
    "        \n",
    "        helper.log_epoch(model, log_entry)\n",
    "        \n",
    "        print(f\"[{params['MODEL_NAME']}] Ep {ep:03d}/{params['EPOCHS']} | Tr Loss: {tr_loss:.4f} | Val Loss: {val_metrics['loss']:.4f} | Val F1-Macro: {monitor_metric_val:.4f}\")\n",
    "\n",
    "        if es.step(monitor_metric_val, model):\n",
    "            print(f\"[INFO] Early stopping triggered at epoch {ep}.\")\n",
    "            break\n",
    "\n",
    "    dur = time.time() - t0\n",
    "    helper.finalize(dur)\n",
    "    \n",
    "    # Reload best model\n",
    "    best_model_path = helper.output_dir / 'best_model.pth'\n",
    "    if best_model_path.exists():\n",
    "        model.load_state_dict(torch.load(best_model_path))\n",
    "        print(f\"[DONE] Final model loaded from best checkpoint (F1-Macro: {helper.best_f1_macro:.4f}).\")\n",
    "\n",
    "    return {'model': model, 'helper': helper}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c427fd6-928e-4940-8a99-2c8b91b12823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CELL 5: Pure Lineage Experiment (Student learns from same-architecture Teacher)\n",
    "# ==============================================================================\n",
    "\n",
    "# CONFIGURATION: Define the architectures for the experiment.\n",
    "ARCHITECTURES = ['resnet18', 'resnet50'] \n",
    "\n",
    "print(f\"[INFO] STARTING PURE LINEAGE EXPERIMENT\")\n",
    "print(f\"[INFO] Architectures to evaluate: {ARCHITECTURES}\")\n",
    "print(\"[INFO] Strategy: Students learn EXCLUSIVELY from their homologous Teacher.\")\n",
    "print(\"[INFO] Validation: Fixed 'Gold Standard' (Original Labeled Data).\")\n",
    "\n",
    "# 1. Prepare \"Gold Standard\" Data (Original Labeled Set)\n",
    "# -----------------------------------------------------------------\n",
    "class_names = sorted([p.name for p in config.TRAIN_VAL_DIR.iterdir() if p.is_dir()])\n",
    "cls2idx = {name: i for i, name in enumerate(class_names)}\n",
    "num_classes = len(class_names)\n",
    "\n",
    "all_original_files, all_original_labels = [], []\n",
    "for class_name in class_names:\n",
    "    class_path = config.TRAIN_VAL_DIR / class_name\n",
    "    files = list(class_path.glob(\"*.png\"))\n",
    "    all_original_files.extend(files)\n",
    "    all_original_labels.extend([cls2idx[class_name]] * len(files))\n",
    "\n",
    "# Fixed Validation Split (20% of original)\n",
    "original_train_files, val_files, original_train_labels, val_labels = train_test_split(\n",
    "    all_original_files, all_original_labels, \n",
    "    test_size=0.2, \n",
    "    random_state=config.SEED, \n",
    "    stratify=all_original_labels\n",
    ")\n",
    "\n",
    "# Common Validation Dataset\n",
    "val_ds = LabeledSpectro(val_files, val_labels, transform=None)\n",
    "print(f\"\\n[GOLD STANDARD] Validation Set: {len(val_ds)} samples\")\n",
    "print(f\"[GOLD STANDARD] Base Training Set: {len(original_train_files)} samples\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# 2. Training Loop per Architecture\n",
    "# -----------------------------------------------------------------\n",
    "for arch in ARCHITECTURES:\n",
    "    teacher_name = arch  # Teacher is same arch\n",
    "    student_arch = arch  # Student is same arch\n",
    "    \n",
    "    print(f\"\\n[INFO] TRAINING LINEAGE: {arch.upper()} (Student {arch} <- Teacher {arch})\")\n",
    "    \n",
    "    # A. Load Verified Pseudo-Labels\n",
    "    hitl_verified_files = []\n",
    "    hitl_verified_labels = []\n",
    "    \n",
    "    # Using config.PSEUDO_LABEL_DIR relative path\n",
    "    pseudo_root = config.PSEUDO_LABEL_DIR / teacher_name\n",
    "    \n",
    "    if not pseudo_root.exists():\n",
    "        print(f\"[WARN] No pseudo-labels found for {teacher_name} at {pseudo_root}. Skipping...\")\n",
    "        continue\n",
    "        \n",
    "    for class_path in pseudo_root.glob('*'):\n",
    "        if class_path.is_dir() and class_path.name in cls2idx:\n",
    "            for file_path in class_path.glob(\"*.png\"):\n",
    "                hitl_verified_files.append(file_path)\n",
    "                hitl_verified_labels.append(cls2idx[class_path.name])\n",
    "    \n",
    "    if len(hitl_verified_files) == 0:\n",
    "        print(f\"[WARN] Pseudo-label directory is empty. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # B. Data Fusion (Original Train + Teacher Pseudo-Labels)\n",
    "    augmented_train_files = original_train_files + hitl_verified_files\n",
    "    augmented_train_labels = original_train_labels + hitl_verified_labels\n",
    "    \n",
    "    train_ds = LabeledSpectro(augmented_train_files, augmented_train_labels, transform=weak_aug)\n",
    "    \n",
    "    print(f\"  > Source Original (Train): {len(original_train_files)}\")\n",
    "    print(f\"  > Source Pseudo-Labels ({teacher_name}): {len(hitl_verified_files)}\")\n",
    "    print(f\"  > TOTAL AUGMENTED DATASET: {len(train_ds)} samples\")\n",
    "    \n",
    "    # C. Configuration\n",
    "    current_params = config.TRAIN_PARAMS[student_arch].copy()\n",
    "    current_params['MODEL_NAME'] = f\"{student_arch}\" \n",
    "    \n",
    "    # Fresh Dataloaders\n",
    "    train_loader = DataLoader(train_ds, batch_size=current_params['BATCH_SIZE'], shuffle=True, num_workers=config.NUM_WORKERS)\n",
    "    val_loader = DataLoader(val_ds, batch_size=current_params['BATCH_SIZE'], shuffle=False, num_workers=config.NUM_WORKERS)\n",
    "    \n",
    "    # D. Execute Training\n",
    "    print(f\"  > Starting training for Student {current_params['MODEL_NAME']}...\")\n",
    "    \n",
    "    results = train_student(current_params, train_loader, val_loader, num_classes, class_names)\n",
    "    \n",
    "    print(f\"[DONE] Lineage {arch} completed.\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n\\n[INFO] EXPERIMENT COMPLETED.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
