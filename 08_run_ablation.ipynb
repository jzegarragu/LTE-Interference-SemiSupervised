{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbf2ee6-1486-4902-8c3c-f3e2d207a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# EXPERIMENT EXECUTION SCRIPT\n",
    "# ==============================================================================\n",
    "# This script trains and evaluates Student models across multiple experimental\n",
    "# scenarios (e.g., Threshold Sensitivity, FixMatch Baseline) defined previously.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix\n",
    "\n",
    "# --- Local Modules ---\n",
    "import config\n",
    "import models\n",
    "import utils\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(config.SEED)\n",
    "np.random.seed(config.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4bf9ff-ebdf-4492-9c86-66bc9de389d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThesisHelper:\n",
    "    \"\"\"\n",
    "    Enhanced helper class to manage experiment logging, checkpointing, and artifacts.\n",
    "    Supports experiment-specific suffixes for organized output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, class_names, base_dir, run_type='teacher'):\n",
    "        self.params = params\n",
    "        self.class_names = class_names\n",
    "        \n",
    "        # Detect experiment suffix from parameters\n",
    "        suffix = params.get('EXPERIMENT_SUFFIX', '') \n",
    "        \n",
    "        # Construct run name with suffix\n",
    "        if run_type == 'student':\n",
    "            self.run_name = f\"student_trained_with_win_teachers{params['MODEL_NAME']}{suffix}\"\n",
    "        else: \n",
    "            self.run_name = f\"{run_type}_{params['MODEL_NAME']}\"\n",
    "            \n",
    "        self.output_dir = Path(base_dir) / self.run_name\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.run_type = run_type \n",
    "        \n",
    "        self.history = []\n",
    "        self.best_f1_macro = -1.0\n",
    "        self.best_epoch_metrics = None\n",
    "        \n",
    "        print(f\"[INFO] ThesisHelper initialized for '{self.run_name}'. Output: {self.output_dir}\")\n",
    "\n",
    "    def log_epoch(self, model, metrics):\n",
    "        self.history.append(metrics)\n",
    "        current_f1_macro = metrics['f1m']\n",
    "        \n",
    "        if current_f1_macro > self.best_f1_macro:\n",
    "            self.best_f1_macro = current_f1_macro\n",
    "            self.best_epoch_metrics = metrics\n",
    "            print(f\"[INFO] New best F1-Macro: {self.best_f1_macro:.4f} (Epoch {metrics['epoch']}). Saving checkpoint...\")\n",
    "            self._save_checkpoint(model)\n",
    "\n",
    "    def _save_checkpoint(self, model):\n",
    "        torch.save(model.state_dict(), self.output_dir / 'best_model.pth')\n",
    "\n",
    "    def finalize(self, total_duration_seconds):\n",
    "        if not self.history:\n",
    "            print(\"[WARN] No history to finalize.\")\n",
    "            return\n",
    "\n",
    "        # 1. Save History\n",
    "        history_df = pd.DataFrame(self.history)\n",
    "        history_df.to_csv(self.output_dir / 'training_history.csv', index=False)\n",
    "        \n",
    "        summary = self.best_epoch_metrics.copy()\n",
    "        summary['total_duration_min'] = total_duration_seconds / 60\n",
    "        cm = summary.pop('cm', None) \n",
    "        \n",
    "        with open(self.output_dir / 'summary.json', 'w') as f:\n",
    "            json.dump(summary, f, indent=4)\n",
    "            \n",
    "        # 2. Generate Artifacts\n",
    "        self._plot_curves(history_df)\n",
    "        self._generate_latex_table(summary, cm)\n",
    "        self._log_to_excel(summary, cm)\n",
    "        \n",
    "        print(f\"[INFO] Experiment finalized. Artifacts saved.\")\n",
    "\n",
    "    def _plot_curves(self, df):\n",
    "        best_epoch = self.best_epoch_metrics['epoch']\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
    "        \n",
    "        # Loss\n",
    "        ax1.plot(df['epoch'], df['tr_loss'], 'o-', label='Training Loss')\n",
    "        ax1.plot(df['epoch'], df['loss'], 'o-', label='Validation Loss')\n",
    "        ax1.axvline(x=best_epoch, color='r', linestyle='--', label=f'Best Epoch ({best_epoch})')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "        \n",
    "        # Metrics\n",
    "        ax2.plot(df['epoch'], df['tr_acc'], 'o-', label='Train Accuracy')\n",
    "        ax2.plot(df['epoch'], df['acc'], 'o-', label='Val Accuracy')\n",
    "        ax2.plot(df['epoch'], df['f1m'], 'o-', label='Val F1-Macro', linewidth=2)\n",
    "        ax2.axvline(x=best_epoch, color='r', linestyle='--')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Metric')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, linestyle='--', alpha=0.6)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir / 'training_curves.png', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def _generate_latex_table(self, summary, cm):\n",
    "        latex_str = f\"\"\"\n",
    "\\\\begin{{table}}[h!]\n",
    "\\\\centering\n",
    "\\\\caption{{Training summary for experiment {self.run_name.replace('_', ' ')}.}}\n",
    "\\\\label{{tab:summary_{self.run_name}}}\n",
    "\\\\begin{{tabular}}{{ll}}\n",
    "\\\\hline\n",
    "Parameter & Value \\\\\\\\\n",
    "\\\\hline\n",
    "Architecture & {self.params['MODEL_NAME']} \\\\\\\\\n",
    "Best Epoch & {summary['epoch']} \\\\\\\\\n",
    "Duration (min) & {summary['total_duration_min']:.2f} \\\\\\\\\n",
    "\\\\hline\n",
    "Validation F1 & {summary['f1m']:.4f} \\\\\\\\\n",
    "Validation Acc & {summary['acc']:.4f} \\\\\\\\\n",
    "\\\\hline\n",
    "\\\\end{{tabular}}\n",
    "\\\\end{{table}}\n",
    "        \"\"\"\n",
    "        with open(self.output_dir / 'summary_table.tex', 'w') as f:\n",
    "            f.write(latex_str)\n",
    "\n",
    "    def _log_to_excel(self, summary, cm):\n",
    "        metrics_to_log = {\n",
    "            'carrier': config.CURRENT_CARRIER,\n",
    "            'model_name': self.params['MODEL_NAME'],\n",
    "            'run_tag': self.run_name,\n",
    "            'num_classes': len(self.class_names),\n",
    "            'acc': summary['acc'],\n",
    "            'loss': summary['loss'],\n",
    "            'f1m': summary['f1m'],\n",
    "            'f1w': summary['f1w'],\n",
    "            'recm': summary['recm'],\n",
    "            'cm': cm,\n",
    "            'epochs': self.params['EPOCHS'],\n",
    "            'batch_size': self.params['BATCH_SIZE'],\n",
    "            'lr': self.params['LR'],\n",
    "            'weight_decay': self.params['WEIGHT_DECAY'],\n",
    "            'notes': f\"Student Exp: {self.run_name}\"\n",
    "        }\n",
    "        utils.log_metrics_excel(config.METRICS_FILE, config.ARTIFACTS_DIR, self.class_names, metrics_to_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db312c30-8772-425c-b633-038f892d139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Classes (Data Augmentation, Dataset, Training Loop) ---\n",
    "\n",
    "class RandomTimeShift(torch.nn.Module):\n",
    "    def __init__(self, max_frac=0.1):\n",
    "        super().__init__()\n",
    "        self.max_frac = max_frac\n",
    "    def forward(self, x):\n",
    "        _, H, W = x.shape\n",
    "        s = int(random.uniform(-self.max_frac, self.max_frac) * W)\n",
    "        return torch.roll(x, shifts=s, dims=-1)\n",
    "\n",
    "class RandomGain(torch.nn.Module):\n",
    "    def __init__(self, a=0.95, b=1.05):\n",
    "        super().__init__()\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "    def forward(self, x):\n",
    "        g = random.uniform(self.a, self.b)\n",
    "        return (x * g).clamp(0, 1)\n",
    "\n",
    "weak_aug = T.Compose([\n",
    "    RandomTimeShift(0.08),\n",
    "    RandomGain(0.95, 1.05),\n",
    "])\n",
    "\n",
    "class LabeledSpectro(Dataset):\n",
    "    def __init__(self, files, labels, transform=None):\n",
    "        self.files = files\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    def __getitem__(self, i):\n",
    "        path = self.files[i]\n",
    "        try:\n",
    "            x = utils.load_png_gray(path)\n",
    "            if self.transform:\n",
    "                x = self.transform(x)\n",
    "            y = self.labels[i]\n",
    "            return x, y\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Skipping file due to error: {e}\")\n",
    "            return torch.zeros(1, config.IMG_SIZE[0], config.IMG_SIZE[1]), self.labels[i]\n",
    "\n",
    "def maybe_resize_for_resnet(x, should_resize):\n",
    "    if should_resize:\n",
    "        return torch.nn.functional.interpolate(x, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "    return x\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience, min_delta, mode='max', restore_best=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.restore_best = restore_best\n",
    "        self.best = -float('inf') if mode == 'max' else float('inf')\n",
    "        self.wait = 0\n",
    "        self.best_state = None\n",
    "\n",
    "    def step(self, metric, model):\n",
    "        is_better = (metric > self.best + self.min_delta) if self.mode == 'max' else (metric < self.best - self.min_delta)\n",
    "        if is_better:\n",
    "            self.best = metric\n",
    "            self.wait = 0\n",
    "            if self.restore_best:\n",
    "                self.best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            return False\n",
    "        self.wait += 1\n",
    "        return self.wait >= self.patience\n",
    "\n",
    "    def restore(self, model):\n",
    "        if self.restore_best and self.best_state is not None:\n",
    "            model.load_state_dict(self.best_state)\n",
    "\n",
    "def evaluate(model, loader, criterion, params):\n",
    "    device = torch.device(config.DEVICE)\n",
    "    model.eval()\n",
    "    va_loss, preds, gts = 0.0, [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = maybe_resize_for_resnet(xb, params.get('RESIZE_224', False))\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            va_loss += loss.item() * xb.size(0)\n",
    "            preds.append(logits.softmax(1).argmax(1).cpu())\n",
    "            gts.append(yb.cpu())\n",
    "    va_loss /= len(loader.dataset)\n",
    "    y_pred = torch.cat(preds).numpy()\n",
    "    y_true = torch.cat(gts).numpy()\n",
    "    \n",
    "    return {\n",
    "        'loss': va_loss,\n",
    "        'acc': accuracy_score(y_true, y_pred),\n",
    "        'f1m': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'f1w': f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'recm': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'cm': confusion_matrix(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "def train_student(params, train_loader, val_loader, num_classes, class_names):\n",
    "    device = torch.device(config.DEVICE)\n",
    "    helper = ThesisHelper(params, class_names, base_dir=config.ARTIFACTS_DIR, run_type='student')\n",
    "\n",
    "    model = models.make_model(\n",
    "        params['MODEL_NAME'], \n",
    "        num_classes, \n",
    "        params.get('USE_PRETRAIN', True)\n",
    "    ).to(device)\n",
    "    \n",
    "    opt = torch.optim.SGD(model.parameters(), lr=params['LR'], momentum=params['MOMENTUM'], weight_decay=params['WEIGHT_DECAY'])\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=params['EPOCHS'], eta_min=params['LR'] * config.ETA_MIN_FACTOR)\n",
    "    \n",
    "    es = EarlyStopping(patience=params['PATIENCE'], min_delta=config.EARLY_STOPPING_CONFIG['min_delta'], restore_best=False)\n",
    "    t0 = time.time()\n",
    "\n",
    "    for ep in range(1, params['EPOCHS'] + 1):\n",
    "        model.train()\n",
    "        tr_loss, n = 0.0, 0\n",
    "        tr_preds, tr_gts = [], []\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb = maybe_resize_for_resnet(xb, params.get('RESIZE_224', False))\n",
    "            xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "            \n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = crit(logits, yb)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=config.CLIP_MAX_NORM)\n",
    "            opt.step()\n",
    "            \n",
    "            tr_loss += loss.item() * xb.size(0)\n",
    "            n += xb.size(0)\n",
    "            tr_preds.append(logits.softmax(1).argmax(1).cpu())\n",
    "            tr_gts.append(yb.cpu())\n",
    "        \n",
    "        tr_loss /= n\n",
    "        sched.step()\n",
    "        tr_acc = accuracy_score(torch.cat(tr_gts).numpy(), torch.cat(tr_preds).numpy())\n",
    "        val_metrics = evaluate(model, val_loader, crit, params)\n",
    "        monitor_val = val_metrics['f1m']\n",
    "        \n",
    "        log_entry = {'epoch': ep, 'tr_loss': tr_loss, 'tr_acc': tr_acc, **val_metrics, 'lr': sched.get_last_lr()[0]}\n",
    "        helper.log_epoch(model, log_entry)\n",
    "        \n",
    "        print(f\"[{params['MODEL_NAME']}] Ep {ep:03d} | TrLoss: {tr_loss:.4f} | ValLoss: {val_metrics['loss']:.4f} | F1: {monitor_val:.4f}\")\n",
    "\n",
    "        if es.step(monitor_val, model):\n",
    "            print(f\"[INFO] Early stopping at epoch {ep}.\")\n",
    "            break\n",
    "\n",
    "    dur = time.time() - t0\n",
    "    helper.finalize(dur)\n",
    "    \n",
    "    # Reload best\n",
    "    best_model_path = helper.output_dir / 'best_model.pth'\n",
    "    if best_model_path.exists():\n",
    "        model.load_state_dict(torch.load(best_model_path))\n",
    "    \n",
    "    return {'model': model, 'helper': helper}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29bbbfa-e591-4c76-8851-4623490a8385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EXPERIMENT CONFIGURATION ---\n",
    "# Mapping experiment folders (from step 07) to descriptive run names\n",
    "RUN_CONFIG = [\n",
    "    {\n",
    "        \"source_folder\": \"PSEUDO_EXP_3.2_FixMatch95\",\n",
    "        \"run_name\": \"EXP_3.2_FixMatch\",\n",
    "        \"desc\": \"Baseline SOTA Automatic (0.95)\"\n",
    "    },\n",
    "    {\n",
    "        \"source_folder\": \"PSEUDO_EXP_3.4_Auto80\",\n",
    "        \"run_name\": \"EXP_3.4_NoHuman\",\n",
    "        \"desc\": \"Human Ablation (Auto 0.80)\"\n",
    "    },\n",
    "    {\n",
    "        \"source_folder\": \"PSEUDO_EXP_3.3_Umb70\",\n",
    "        \"run_name\": \"EXP_3.3_Thresh70\",\n",
    "        \"desc\": \"Sensitivity Analysis (0.70)\"\n",
    "    },\n",
    "    {\n",
    "        \"source_folder\": \"PSEUDO_EXP_3.3_Umb90\",\n",
    "        \"run_name\": \"EXP_3.3_Thresh90\",\n",
    "        \"desc\": \"Sensitivity Analysis (0.90)\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Paths\n",
    "BASE_EXP_DIR = config.ARTIFACTS_DIR / \"ABLATION_EXPERIMENTS\"\n",
    "CLASS_NAMES = sorted([p.name for p in config.TRAIN_VAL_DIR.iterdir() if p.is_dir()])\n",
    "CLS2IDX = {name: i for i, name in enumerate(CLASS_NAMES)}\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "# 1. LOAD ORIGINAL DATASET (Train/Val Fixed Split)\n",
    "# Use the same seed to ensure the Validation Set is identical to previous stages\n",
    "all_original_files, all_original_labels = [], []\n",
    "for class_name in CLASS_NAMES:\n",
    "    class_path = config.TRAIN_VAL_DIR / class_name\n",
    "    files = list(class_path.glob(\"*.png\"))\n",
    "    all_original_files.extend(files)\n",
    "    all_original_labels.extend([CLS2IDX[class_name]] * len(files))\n",
    "\n",
    "original_train_files, val_files, original_train_labels, val_labels = train_test_split(\n",
    "    all_original_files, all_original_labels, \n",
    "    test_size=0.2, \n",
    "    random_state=config.SEED, \n",
    "    stratify=all_original_labels\n",
    ")\n",
    "\n",
    "val_ds = LabeledSpectro(val_files, val_labels, transform=None) \n",
    "print(f\"[DATA] Fixed Validation Set: {len(val_ds)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eba6705-ab66-4d25-a2d5-69461832f4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EXPERIMENT EXECUTION LOOP ---\n",
    "for run in RUN_CONFIG:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TESTING: {run['run_name']}\")\n",
    "    print(f\"Source: {run['source_folder']}\")\n",
    "    print(f\"Goal: {run['desc']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # A. Load Pseudo-Labels\n",
    "    pseudo_path = BASE_EXP_DIR / run['source_folder']\n",
    "    if not pseudo_path.exists():\n",
    "        print(f\"[ERROR] Path not found: {pseudo_path}. Please run notebook 07 first.\")\n",
    "        continue\n",
    "        \n",
    "    pseudo_files, pseudo_labels = [], []\n",
    "    for class_path in pseudo_path.glob('*'):\n",
    "        if class_path.is_dir() and class_path.name in CLS2IDX:\n",
    "            for file_path in class_path.glob(\"*.png\"):\n",
    "                pseudo_files.append(file_path)\n",
    "                pseudo_labels.append(CLS2IDX[class_path.name])\n",
    "    \n",
    "    print(f\"   > Loaded Pseudo-Labels: {len(pseudo_files)}\")\n",
    "    \n",
    "    # B. Combine Datasets\n",
    "    aug_files = original_train_files + pseudo_files\n",
    "    aug_labels = original_train_labels + pseudo_labels\n",
    "    train_ds = LabeledSpectro(aug_files, aug_labels, transform=weak_aug)\n",
    "    \n",
    "    batch_size = config.TRAIN_PARAMS['resnet50']['BATCH_SIZE']\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=config.NUM_WORKERS)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=config.NUM_WORKERS)\n",
    "    \n",
    "    # C. Configure Training\n",
    "    current_params = config.TRAIN_PARAMS['resnet50'].copy()\n",
    "    current_params['MODEL_NAME'] = 'resnet50'\n",
    "    # Important: Suffix ensures folder uniqueness\n",
    "    current_params['EXPERIMENT_SUFFIX'] = f\"_{run['run_name']}\" \n",
    "    \n",
    "    # D. Train\n",
    "    print(f\"   > Training ResNet50 Student...\")\n",
    "    train_student(current_params, train_loader, val_loader, NUM_CLASSES, CLASS_NAMES)\n",
    "    \n",
    "    print(f\"[DONE] {run['run_name']} Finished.\")\n",
    "\n",
    "print(\"\\n--- ALL EXPERIMENTS COMPLETED ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9e43ca-6ae3-4110-9943-64498e0bbdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FINAL EVALUATION ON OFFICIAL TEST SET\n",
    "# ==============================================================================\n",
    "# This step assesses all experimental models against the strictly isolated Test Set.\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(\"FINAL THESIS EVALUATION (Official Test Set)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# 1. Prepare Test Set\n",
    "TEST_DIR = config.TEST_DIR\n",
    "if not TEST_DIR.exists():\n",
    "     raise FileNotFoundError(f\"[CRITICAL] Test directory not found at {TEST_DIR}\")\n",
    "\n",
    "print(f\"[DATA] Loading Test Set from: {TEST_DIR}\")\n",
    "\n",
    "test_files, test_labels = [], []\n",
    "for class_name in CLASS_NAMES:\n",
    "    c_path = TEST_DIR / class_name\n",
    "    if not c_path.exists(): continue\n",
    "    files = list(c_path.glob(\"*.png\"))\n",
    "    test_files.extend(files)\n",
    "    test_labels.extend([CLS2IDX[class_name]] * len(files))\n",
    "\n",
    "print(f\"[DATA] Total Test Images: {len(test_files)}\")\n",
    "\n",
    "test_ds = LabeledSpectro(test_files, test_labels, transform=None)\n",
    "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, num_workers=config.NUM_WORKERS)\n",
    "\n",
    "# 2. Evaluation Loop\n",
    "results_thesis = []\n",
    "device = torch.device(config.DEVICE)\n",
    "\n",
    "def evaluate_final(model, loader):\n",
    "    model.eval()\n",
    "    preds, gts = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = torch.nn.functional.interpolate(xb, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            preds.append(logits.softmax(1).argmax(1).cpu())\n",
    "            gts.append(yb.cpu())\n",
    "    y_pred = torch.cat(preds).numpy()\n",
    "    y_true = torch.cat(gts).numpy()\n",
    "    return {\n",
    "        'acc': accuracy_score(y_true, y_pred),\n",
    "        'f1m': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'recm': recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    }\n",
    "\n",
    "for run in RUN_CONFIG:\n",
    "    print(f\"\\nEvaluating Experiment: {run['run_name']}...\")\n",
    "    \n",
    "    folder_name = f\"student_trained_with_win_teachersresnet50_{run['run_name']}\"\n",
    "    model_path = config.ARTIFACTS_DIR / folder_name / \"best_model.pth\"\n",
    "    \n",
    "    if not model_path.exists():\n",
    "        print(f\"[ERROR] Model not found at {model_path}\")\n",
    "        continue\n",
    "        \n",
    "    model = models.make_model('resnet50', len(CLASS_NAMES), resnet_use_pretrain=False)\n",
    "    \n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))\n",
    "    except TypeError:\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        \n",
    "    model.to(device)\n",
    "    \n",
    "    metrics = evaluate_final(model, test_loader)\n",
    "    print(f\"   -> Test F1-Macro: {metrics['f1m']:.4f}\")\n",
    "    \n",
    "    results_thesis.append({\n",
    "        \"Experiment\": run['run_name'],\n",
    "        \"Strategy\": run['desc'],\n",
    "        \"Test_F1_Macro\": metrics['f1m'],\n",
    "        \"Test_Accuracy\": metrics['acc'],\n",
    "        \"Test_Recall\": metrics['recm']\n",
    "    })\n",
    "\n",
    "# 3. Save Final CSV\n",
    "df_final = pd.DataFrame(results_thesis)\n",
    "print(\"\\n=== FINAL THESIS RESULTS ===\")\n",
    "print(df_final)\n",
    "\n",
    "output_csv = config.ARTIFACTS_DIR / \"FINAL_COMPARATIVE_TABLE_OFFICIAL_TEST.csv\"\n",
    "df_final.to_csv(output_csv, index=False)\n",
    "print(f\"\\n[DONE] Results saved to: {output_csv}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
